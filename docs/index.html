<!doctype html>
<html lang="en">
  <head>
	<meta name="generator" content="Hugo 0.58.3" />
    <meta charset="utf-8">
<title>Tutorial: Debug Your Kubernetes Apps - Arun Gupta &amp; Re Alvarez Parmar, Amazon</title>


<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="/debug-k8s-apps/reveal-js/css/reset.css">
<link rel="stylesheet" href="/debug-k8s-apps/reveal-js/css/reveal.css"><link rel="stylesheet" href="/debug-k8s-apps/reveal-js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="/debug-k8s-apps/highlight-js/default.min.css">
    <link rel="shortcut icon" href="images/favicon.ico" >

  </head>
  <body>
    <div class="reveal">
      <div class="slides">
  

    <section>

<style type="text/css">
  .reveal {
    font-size: 30px;
  }
  .reveal p {
    text-align: left;
    font-size: 25px;
  }
  .reveal h3 {
    text-align: left;
  }
  .reveal ul {
    display: block;
    font-size: 25px;
  }
  .reveal ol {
    display: block;
    font-size: 25px;
  }
  .reveal code {
   font-size: 20px;
   font-face: "courier";
   color: #1A120B;
  } 
  .reveal pre code {
   font-size: 15px;
  }
</style>

<h1 id="tutorial-debug-your-kubernetes-apps">Tutorial: Debug Your Kubernetes Apps</h1>

<h4 id="arun-gupta-re-alvarez-parmar-amazon">Arun Gupta &amp; Re Alvarez Parmar, Amazon</h4>

</section><section>

<p>In this presentation we will learn how to troubleshoot Kubernetes applications.</p>

</section><section>

<p>Agenda slide</p>

</section><section>

<p><strong>EKS Architecture</strong></p>

<p><img src="images/eks-arch.jpg" alt="" /></p>

</section><section>

<p><strong>How to setup a Kubernetes cluster</strong></p>

<p><span class='fragment '
  >
   1. Create Master Nodes (aka the Control Plane)
</span></p>

<p><span class='fragment '
  >
   2. use <em>kubectl</em> to connect to the Control Plane
</span></p>

<p><span class='fragment '
  >
   3. Install Worker nodes
</span></p>

<p><span class='fragment '
  >
   4. Deploy apps and add-ons
</span></p>

<p><span class='fragment '
  >
   5. &hellip; profit?
</span></p>
</section>

  

    <section>

<h3 id="eks-architecture">EKS architecture</h3>

<ul>
<li>AWS Managed Control Plane

<ul>
<li>Master nodes</li>
<li>etcd cluster nodes</li>
<li>NLB for API load-balancing</li>
</ul></li>
<li>Highly available</li>
<li>AWS IAM authentication</li>
<li>VPC networking</li>
</ul>

</section><section>

<h3 id="eks-core-tenets">EKS core tenets</h3>

<ul>
<li>Platform for enterprises to run production grade workloads</li>
<li>Provide a native and upstream experience (CNCF Certified)</li>
<li>Provide seamless integration with AWS services</li>
<li>Actively contribute to upstream project</li>
</ul>

</section><section>

<h3 id="kubernetes-components">Kubernetes Components</h3>

<ul>
<li>Master node</li>
<li>Worker Node</li>
<li>kubectl (User)</li>
</ul>

</section><section>

<h3 id="master-node-components">Master node components</h3>

<ul>
<li><strong>apiserver:</strong> exposes APIs for  master nodes</li>
<li><strong>scheduler:</strong> decides which pod should run on which worker node</li>
<li><strong>controller manager:</strong> makes changes attempting to move the current state towards the desired state</li>
<li><strong>etcd:</strong> key/value data store used to store cluster state</li>
</ul>

</section><section>

<h3 id="etcd-design">etcd design</h3>

<ul>
<li>Minimum 3 etcd servers</li>
<li>Spread across availability zones</li>
<li>Uses RAFT protocol</li>
</ul>

</section><section>

<h3 id="worker-node-components">Worker node components</h3>

<ul>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/"><strong>kubelet:</strong></a> handles communication between worker and master nodes</li>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/"><strong>kube-proxy:</strong></a> handles communication between pods, nodes, and the outside world</li>
<li><strong>container runtime:</strong> runs containers on the node.</li>
</ul>

</section><section>

<h3 id="network-considerations">Network considerations</h3>

<ul>
<li>EKS cluster endpoint can be public or private</li>
<li>EKS uses aws-vpc-cni</li>
<li>Worker nodes and Pods get VPC IP</li>
</ul>

</section><section>

<h3 id="aws-vpc-cni-https-github-com-aws-amazon-vpc-cni-k8s"><a href="https://github.com/aws/amazon-vpc-cni-k8s">aws-vpc-cni</a></h3>

<ul>
<li>Pods recieve an IP address from a VPC subnet</li>
<li>Max number of pods is limited by EC2 Instance size</li>
<li>No IP = Pod Pending</li>
<li>Plan for growth</li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-env-vars.html">customize cni variables</a></li>
</ul>

</section><section>

<h1 id="use-cni-metrics-helper-https-docs-aws-amazon-com-eks-latest-userguide-cni-metrics-helper-html"><a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-metrics-helper.html">use CNI Metrics Helper</a></h1>

<p><img src="https://docs.aws.amazon.com/eks/latest/userguide/images/EKS_CNI_metrics.png" alt="" /></p>

</section><section>

<h3 id="kubelet-resource-reservation">Kubelet resource reservation</h3>

<ul>
<li><p>Monitor kubelet on the worker node</p>

<pre><code>journalctl -u kubelet
</code></pre></li>

<li><p>Use kube-reserved to reserve resources for kubelet, container runtime &amp; node problem detector</p>

<pre><code>--kube-reserved=[cpu-100m][,][memory=100Mi][,]
[ephemeral-storage=1Gi][,][pid=1000]
</code></pre></li>

<li><p>Use system-reserved to reserve resources for system daemons liks sshd, udev, kernel</p>

<pre><code>--system-reserved=[cpu-100m][,][memory=100Mi][,]
[ephemeral-storage=1Gi][,][pid=1000]
</code></pre></li>
</ul>

</section><section>

<h3 id="coredns-scaling">Coredns scaling</h3>

<blockquote>
<p>coreDNSMemory required in MB = (Pods + Services)/1000 + 54</p>
</blockquote>

<ul>
<li><p>Scale CoreDNS pods</p>

<pre><code>kubectl -n kube-system scale --current-replicas=2
--replicas=10 deployment/coredns
</code></pre></li>

<li><p>Node-local DNS <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns">addon</a></p>

<ul>
<li>CoreDNS DaemonSet on each node</li>
</ul></li>
</ul>
</section>
    

<section data-noprocess>
 <h2>EKS Architecture</h2>
 <img src="https://d1.awsstatic.com/Getting%20Started/eks-project/EKS-demo-app.e7ce7b188f2662b8573b5881a6b843e09caf729a.png" height=400>
</section>

</section><section>

<section data-markdown>
### How does kubectl work
  <script>
### How does kubectl work

- kubectl communicates with the Kubernetes API server <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="1" --> 
- uses a configuration file generally located at ~/.kube/config <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="2" -->
- In EKS, kubectl + aws-iam-authenticator = ‚ù§Ô∏è <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="3" -->
  </script>
</section>

</section><section>

<h3 id="if-your-kubectl-cannnot-connect-to-your-kubernetes-cluster">If your kubectl cannnot connect to your Kubernetes cluster</h3>

<p><span class='fragment '
  >
  1. Update <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a> &amp; <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">aws-iam-authenticator</a>
</span></p>

<p><span class='fragment '
  >
  2. Update kubeconfig using AWS cli<br />
</span></p>

<p><span class='fragment '
  >
  3. Is cluster endpoint accessible?<br />
</span></p>

</section><section>

<h3 id="check-if-cluster-is-accessible">Check if cluster is accessible</h3>

<pre><code>curl -k http://CLUSTER_ENDPOINT/api/v1
</code></pre>

<p>response:</p>

<pre><code>&quot;kind&quot;: &quot;APIResourceList&quot;,
  &quot;groupVersion&quot;: &quot;v1&quot;,
  &quot;resources&quot;: [
  {
        &quot;name&quot;: &quot;bindings&quot;,
              &quot;singularName&quot;: &quot;&quot;,
                    &quot;namespaced&quot;: true,
                          &quot;kind&quot;: &quot;Binding&quot;,
                          &quot;verbs&quot;: [
                                  &quot;create&quot;
...                                        
</code></pre>

</section><section>

<h4 id="use-aws-cli-to-auto-generate-kube-config-file">Use AWS CLI to auto-generate kube config file</h4>

<pre><code>aws eks update-kubeconfig --name {cluster-name} 
</code></pre>

</section><section>

<p><em>cat ~/.kube/config</em></p>

<pre><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: {REDACTED} 
    server: https://DFEA886AB17A069545SJDS9F06BCE3DCC.gr7.us-west-2.eks.amazonaws.com
  name: arn:aws:eks:us-west-2:09123456789:cluster/eks1
contexts:
- context:
    cluster: arn:aws:eks:us-west-2:09123456789:cluster/eks1
    user: arn:aws:eks:us-west-2:09123456789:cluster/eks1
  name: arn:aws:eks:us-west-2:09123456789:cluster/eks1
current-context: arn:aws:eks:us-west-2:09123456789:cluster/eks1
kind: Config
preferences: {}
users:
	- name: arn:aws:eks:us-west-2:09123456789:cluster/eks1
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - token
      - -i
      - eks1
      command: aws-iam-authenticator
</code></pre>

</section><section>

<h3 id="aws-auth-config-map">aws-auth config map</h3>

<pre><code>kubectl -n kube-system describe configmap aws-auth
</code></pre>

<pre><code>Name:         aws-auth
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
mapRoles:
----
- groups:
  - system:bootstrappers
  - system:nodes
  rolearn: arn:aws:iam::09123456789:role/eksctl-eks-nodegroup-ng
  username: system:node:{{EC2PrivateDNSName}}

mapUsers:
----
- userarn: arn:aws:iam::09123456789:user/realvarez
  groups:
    - system:masters

</code></pre>

</section><section>

<h3 id="kubectl-works">kubectl works!</h3>

<pre><code>kubectl cluster-info
</code></pre>

<p>output:</p>

<pre><code>Kubernetes master is running at https://xxxx.y.region.eks.amazonaws.com
</code></pre>
</section>
    <section>

<h1 id="pod-is-pending">Pod is pending</h1>

</section><section>

<p>You&rsquo;ve run 8 replicas of a pod:</p>

<pre><code>kubectl create -f hello-deployment.yaml 
deployment.apps/hello created
</code></pre>

<p>and scaled to 8 replicas:</p>

<pre><code>$ kubectl scale --replicas=8 deployment hello 
deployment.extensions/hello scaled
</code></pre>

<p>Deployment shows only 4 replicas are available:</p>

<pre><code>$ kubectl get deployments
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
hello   4/8     8            4           23s
</code></pre>

</section><section>

<p>This is matched by the output of <code>get pods</code>:</p>

<pre><code>$ kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
hello-67c5c968fd-2s4n7   1/1     Running   0          64m
hello-67c5c968fd-5lzxw   0/1     Pending   0          64m
hello-67c5c968fd-894z6   1/1     Running   0          64m
hello-67c5c968fd-gv9cw   0/1     Pending   0          64m
hello-67c5c968fd-lwww8   0/1     Pending   0          64m
hello-67c5c968fd-p8mxd   1/1     Running   0          64m
hello-67c5c968fd-vlnsd   1/1     Running   0          64m
hello-67c5c968fd-wj6j8   0/1     Pending   0          64m
</code></pre>

</section><section>

<p>Multiple reasons:</p>

<ul>
<li>Not enough resources in the cluster

<ul>
<li>CPU, memory, port</li>
</ul></li>
<li>Security group does not have an ingress rule with 443 port access</li>
<li>Ensure all nodes are healthy</li>
</ul>

</section><section>

<p>Describe the pod:</p>

<pre><code>kubectl describe pod/hello-67c5c968fd-5lzxw
</code></pre>

<p>Shows the output:</p>

<pre><code>. . .

Events:
  Type     Reason            Age                    From               Message
  ----     ------            ----                   ----               -------
  Warning  FailedScheduling  107s (x172 over 147m)  default-scheduler  0/4 nodes are available: 4 Insufficient cpu.
</code></pre>

<p>Events are only visible on pods, not on Deployments, ReplicaSet, Job, or any other resource that created pod.</p>

</section><section>

<p>Alternatively, get all events:</p>

<pre><code>$ kubectl get events 
LAST SEEN   TYPE      REASON             KIND   MESSAGE
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
</code></pre>

</section><section>

<p>Or only the warning events:</p>

<pre><code>kubectl get events --field-selector type=Warning
</code></pre>

</section><section>

<p>Let&rsquo;s get events only for the pod:</p>

<pre><code>$ kubectl get events --field-selector involvedObject.kind=Pod,involvedObject.name=hello-67c5c968fd-5lzxw
LAST SEEN   TYPE      REASON             KIND   MESSAGE
4m41s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
</code></pre>

</section><section>

<p>Sort by timestamp:</p>

<pre><code>$ kubectl get events --sort-by='.lastTimestamp'
</code></pre>

</section><section>

<p>Check memory/CPU requirements of pod:</p>

<pre><code>kubectl describe deployments/hello
</code></pre>

<p>Output:</p>

<pre><code>  Containers:
   hello:
    Image:      nginx:latest
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Limits:
      cpu:     2
      memory:  2000Mi
    Requests:
      cpu:        2
      memory:     2000Mi
    Environment:  &lt;none&gt;
</code></pre>

<p>Default CPU request is <code>200m</code> and none on memory. There are no limits.</p>

<p>1000m (milicores) = 1 core = 1 CPU = 1 AWS vCPU.</p>

<p>In this case, CPU request and limits have been specified to <code>2</code> and memory to <code>2GB</code>. So we need 8 blocks of 2 CPU and 2 GB memory.</p>

</section><section>

<p>Check memory/CPU available in cluster:</p>

<pre><code>$ kubectl top nodes
Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)
</code></pre>

</section><section>

<p>Install metrics-server:</p>

<pre><code>curl -OL https://github.com/kubernetes-sigs/metrics-server/archive/v0.3.6.tar.gz
tar xzvf v0.3.6.tar.gz
kubectl create -f metrics-server-0.3.6/deploy/1.8+/
</code></pre>

</section><section>

<p>Get memory/CPU for nodes:</p>

<pre><code>$ kubectl top nodes
NAME                                           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-192-168-28-108.us-west-2.compute.internal   28m          0%     410Mi           2%        
ip-192-168-48-190.us-west-2.compute.internal   33m          0%     363Mi           2%        
ip-192-168-51-148.us-west-2.compute.internal   29m          0%     338Mi           2%        
ip-192-168-64-166.us-west-2.compute.internal   32m          0%     395Mi           2% 
</code></pre>

</section><section>

<p>Get available memory on each node:</p>

<pre><code>$ kubectl get no -o json | jq -r '.items | sort_by(.status.capacity.memory)[]|[.metadata.name,.status.allocatable.memory]| @tsv'
ip-192-168-28-108.us-west-2.compute.internal	15950552Ki
ip-192-168-48-190.us-west-2.compute.internal	15950552Ki
ip-192-168-51-148.us-west-2.compute.internal	15950552Ki
ip-192-168-64-166.us-west-2.compute.internal	15950552Ki
</code></pre>

<p>And do the same for CPU:</p>

<pre><code>$ kubectl get no -o json | jq -r '.items | sort_by(.status.capacity.memory)[]|[.metadata.name,.status.allocatable.cpu]| @tsv'
ip-192-168-28-108.us-west-2.compute.internal	4
ip-192-168-48-190.us-west-2.compute.internal	4
ip-192-168-51-148.us-west-2.compute.internal	4
ip-192-168-64-166.us-west-2.compute.internal	4
</code></pre>

<p>So, there is enough memory but not CPU.</p>

</section><section>

<p>Create cluster autoscaler:</p>

<pre><code></code></pre>

</section><section>

<p>Create horizontal pod autoscaler</p>

<pre><code></code></pre>

</section><section>

</section><section>

<pre><code>$ kubectl get pods -l app=mnist,type=inference
NAME                             READY   STATUS    RESTARTS   AGE
mnist-inference-cd78cfd5-hcvfd   0/1     Pending   0          3m48s
</code></pre>

<pre><code>kubectl describe pod mnist-inference-cd78cfd5-hcvfd

. . .

Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  3s (x8 over 5m32s)  default-scheduler  0/2 nodes are available: 2 Insufficient nvidia.com/gpu.
</code></pre>
</section>
    <section>

<h1 id="what-about-stateful-containers">What about stateful containers?</h1>

</section><section>

<h3 id="pods-are-ephemeral-by-nature">Pods are ephemeral by nature</h3>

<ul>
<li><p>Volumes allow pods to persist data</p></li>

<li><p>Volumes are accessible to all containers in a pod</p></li>

<li><p>Data can persist even after pod termination</p></li>
</ul>

</section><section>

<h3 id="persistentvolume-pv">PersistentVolume (PV)</h3>

<blockquote>
<p>pre-provisioned storage in the cluster or dynamically provisioned by storage class</p>
</blockquote>

<h3 id="persistentvolumeclaim-pvc">PersistentVolumeClaim (PVC)</h3>

<blockquote>
<p>request for storage by a user</p>
</blockquote>

<h3 id="storageclass">StorageClass</h3>

<blockquote>
<p>administrator provided &ldquo;classes&rdquo; of storage</p>
</blockquote>

<aside class="notes">
Persistent Volumes provide a plugin model for storage in Kubernetes. How storage is provided is abstracted from how it is consumed

</aside>

</section><section>

<h3 id="what-are-statefulsets">What are statefulsets?</h3>

<ul>
<li>Provide pods with storage to persist data</li>
<li>Create PVC dynamically using <code>volumeClaimTemplates</code></li>
<li>Each pod gets its own dedicated PVC</li>
<li>Create headless service type (<code>clusterIP: None</code>)</li>
<li>Ordered deployment and scaling
<aside class="notes">
‚ÄúAs you probably realize, we said PVCs are created and associated with the Pods, but we didn‚Äôt say anything about PVs. That is because StatefulSets do not manage PVs in any way. The storage for the Pods must be provisioned in advance by an admin, or provisioned on-demand by a PV provisioner based on the requested storage class and ready for consumption by the stateful Pods.‚Äù
<aside class="notes"></section><section></li>
</ul>

<h3 id="step-1-create-storage-class">Step 1. Create storage class</h3>

<pre><code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: mysql-gp2
  provisioner: kubernetes.io/aws-ebs
  parameters:
    type: gp2
    reclaimPolicy: Delete
    mountOptions:
    - debug
</code></pre>

</section><section>

<h3 id="step-2-use-volumeclaimtemplates-in-the-statefulset">Step 2. use volumeClaimTemplates in the Statefulset</h3>

<pre><code>volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: mysql-gp2
      resources:
        requests:
          storage: 10Gi
</code></pre>

</section><section>

<h3 id="mysql-statefulsets-on-kubernetes">mysql statefulsets on Kubernetes</h3>

<pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  spec:
    selector:
      matchLabels:
        app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
   ...

        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql

  ...
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: mysql-gp2
      resources:
        requests:
          storage: 10Gi
</code></pre>

<p>See full yaml <a href="https://eksworkshop.com/statefulset/statefulset.files/mysql-statefulset.yml">here</a></p>

</section><section>

<h3 id="persistent-storage-options-in-eks">Persistent storage options in EKS</h3>

<ul>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html">EBS CSI Driver</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html">EFS CSI Driver</a></li>
<li><a href="https://github.com/kubernetes-sigs/aws-fsx-csi-driver">FSx for Lustre CSI Driver (Alpha)</a></li>
</ul>

</section><section>

<h3 id="eks-ebs">EKS + EBS</h3>

<p><ul>
<li>Persistent volumes can be <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/dynamic-provisioning">dynamically</a> or
<a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/static-provisioning">statically</a> provisioned</li>
<li>Volumes can be <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/resizing">resized</a> and support <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/snapshot">Snapshots</a></li>
<li>Volumes are local to AZ, use EFS where possible</li>
</ul>
</aside></aside></p>
</section>
    <section>

<h1 id="going-public">Going public</h1>

</section><section>

<h3 id="options-for-exposing-kubernetes-apps">Options for exposing Kubernetes apps</h3>

<ul>
<li><p>ServiceType=LoadBalancer. Uses Classic load balancer.</p>

<ul>
<li><p>Use Network load balancer, annotate your service:</p>

<pre><code>service.beta.kubernetes.io/aws-load-balancer-type: nlb
</code></pre></li>
</ul></li>

<li><p>Ingress using <a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller">alb-ingresss-controller</a></p>

<ul>
<li><p>For internal only load balancer, annotate your service:</p>

<pre><code>service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0

</code></pre></li>
</ul></li>

<li><p><a href="https://github.com/kubernetes/ingress-nginx">NGINX Ingress Controller</a></p></li>
</ul>

</section><section>

<h3 id="if-load-balancer-fails-to-deploy">If load balancer fails to deploy</h3>

<ul>
<li>Check <a href="https://docs.aws.amazon.com/en_pv/eks/latest/userguide/network_reqs.html">tags</a></li>

<li><p>Check service</p>

<pre><code>kubectl describe service
</code></pre></li>

<li><p>Check <a href="https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html">EKS IAM Service Role</a></p></li>
</ul>

</section><section>

<h3 id="troubleshoot-latency-in-app">Troubleshoot latency in app</h3>

<ul>
<li>Check <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html">CloudWatch Metrics for load balancer</a></li>

<li><p>Check latency within cluster</p>

<pre><code>kubectl apply -f https://k8s.io/examples/application/shell-demo.yaml

kubectl exec -it shell-demo -- /bin/bash

root@shell-demo:/# apt-get update 
root@shell-demo:/# apt-get install curl
root@shell-demo:/# curl http://10.100.2.39 &lt;-- IP of the service
</code></pre></li>

<li><p>Implement <a href="https://docs.aws.amazon.com/whitepapers/latest/microservices-on-aws/distributed-monitoring.html">tracing</a>
preferably with a <a href="https://aws.amazon.com/app-mesh/">Service Mesh</a></p></li>
</ul>
</section>
    <section>

<h1 id="how-to-find-problems">How to find problems?</h1>

</section><section>

<h3 id="monitoring-tools">üéõ Monitoring tools</h3>

<ul>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/prometheus.html">Prometheus</a> + <a href="https://eksworkshop.com/monitoring/">Grafana</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/deploy-container-insights-EKS.html">CloudWatch Container Insights</a></li>
<li>Kubernetes <a href="https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html">Dashboard</a></li>
<li>Commercial solutions (e.g., Datadog, Dynatrace, Sysdig, etc.)</li>
</ul>

<aside class="notes">
Prometheus provides metrics storage. Grafana provides dashboards. 
</aside>

</section><section>

<section data-markdown>
### Key things to monitor
  <script> 
  ### Key things to monitor
- Nodes <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="1" -->
   - CPU/memory usage, disk pressure & network <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="1" -->
- Pods, deployments and services <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="2" -->
   - Pod count <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="2" -->
   - Container health checks <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="2" -->
- coreDNS <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="3" -->
   - DNS latency <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="3" -->
  </script>
</section>

</section><section>

<h3 id="container-monitoring-options">Container monitoring options</h3>

<ul>
<li>Readiness probe</li>
<li>Liveness probe</li>
<li>Tracing (OpenTracing, AWS X-ray)</li>
<li>Process health</li>
<li>Process metrics</li>
<li>Process logs</li>
</ul>

<aside class="notes">
A good cloud native container must provide APIs for runtime to monitor the health, if checks fail, an action should be triggered.
</aside>

</section><section>
</section>
    <section>

<h1 id="let-s-talk-about-hpa">Let&rsquo;s talk about HPA</h1>

</section><section>

<p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a> (HPA) scales the pods in a deployment or replica set. It is implemented as a K8s API resource and a controller. The controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. It obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).</p>

<p>Default HPA loop is 15 seconds (controlled by <code>--horizontal-pod-autoscaler-sync-period</code>)</p>

<p>Install metrics server:</p>

<pre><code>helm install stable/metrics-server \
    --name metrics-server \
    --version 2.0.4 \
    --namespace metrics
</code></pre>

<p>Deploy a sample app:</p>

<pre><code>kubectl run php-apache --generator=run-pod/v1 --image=k8s.gcr.io/hpa-example --requests=cpu=200m --expose --port=80
</code></pre>

<p>Get all the objects:</p>

<pre><code>kubectl get all
NAME             READY   STATUS    RESTARTS   AGE
pod/php-apache   1/1     Running   0          31s

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1      &lt;none&gt;        443/TCP   23h
service/php-apache   ClusterIP   10.100.150.64   &lt;none&gt;        80/TCP    31s
</code></pre>

<p>Watch pod stats:</p>

<pre><code>watch kubectl top pod 
</code></pre>

<p>HPA fetches metrics from <code>metrics.k8s.io</code> that is provided by metrics server. It needs to be installed separately.</p>
</section>
    <section><p>Thank you!</p>
</section>

</div>
      
    </div>
<script type="text/javascript" src=/debug-k8s-apps/reveal-hugo/object-assign.js></script>

<a href="/debug-k8s-apps/reveal-js/css/print/" id="print-location" style="display: none;"></a>
<script type="text/javascript">
  var printLocationElement = document.getElementById('print-location');
  var link = document.createElement('link');
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = printLocationElement.href + (window.location.search.match(/print-pdf/gi) ? 'pdf.css' : 'paper.css');
  document.getElementsByTagName('head')[0].appendChild(link);
</script>

<script type="application/json" id="reveal-hugo-site-params">{"controls":false,"custom_theme":"stylesheets/robot-lung.css","custom_theme_compile":true,"slide_number":true}</script>
<script type="application/json" id="reveal-hugo-page-params">{"controls":false,"hash":false,"height":"80%","margin":0.1,"progress":false,"slide_number":true,"theme":"white","transition":"fade","width":"80%"}</script>

<script src="/debug-k8s-apps/reveal-js/js/reveal.js"></script>

<script type="text/javascript">
  
  
  function camelize(map) {
    if (map) {
      Object.keys(map).forEach(function(k) {
        newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
        if (newK != k) {
          map[newK] = map[k];
          delete map[k];
        }
      });
    }
    return map;
  }
  
  var revealHugoDefaults = { center: true, controls: true, history: true, progress: true, transition: "slide" };
  var revealHugoSiteParams = JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);
  var revealHugoPageParams = JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);
  
  var options = Object.assign({},
    camelize(revealHugoDefaults),
    camelize(revealHugoSiteParams),
    camelize(revealHugoPageParams));
  Reveal.initialize(options);
</script>


  
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/markdown/marked.js"></script>
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/markdown/markdown.js"></script>
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/highlight/highlight.js"></script>
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/zoom-js/zoom.js"></script>
  
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/notes/notes.js"></script>



    <style>
  #logo {
    position: absolute;
    top: 10px;
    left: 10px;
    width: 10%;
  }
</style>

<img id="logo" src="images/aws-logo.svg" alt="AWS Smile!">

    
  </body>
</html>
