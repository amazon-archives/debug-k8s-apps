<!doctype html>
<html lang="en">
  <head>
	<meta name="generator" content="Hugo 0.58.3" />
    <meta charset="utf-8">
<title>Tutorial: Debug Your Kubernetes Apps - Arun Gupta &amp; Re Alvarez Parmar, Amazon</title>


<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="/debug-k8s-apps/reveal-js/css/reset.css">
<link rel="stylesheet" href="/debug-k8s-apps/reveal-js/css/reveal.css"><link rel="stylesheet" href="/debug-k8s-apps/reveal-js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="/debug-k8s-apps/highlight-js/default.min.css">
    <link rel="shortcut icon" href="images/favicon.ico" >

  </head>
  <body>
    <div class="reveal">
      <div class="slides">
  

    <section>

<style type="text/css">
  .reveal {
    font-size: 30px;
  }
  .reveal p {
    text-align: left;
    font-size: 25px;
  }
  .reveal h3 {
    text-align: left;
  }
  .reveal ul {
    display: block;
    font-size: 25px;
  }
  .reveal ol {
    display: block;
    font-size: 25px;
  }
  .reveal code {
   font-size: 15px;
  } 
  .reveal pre code {
   font-size: 15px;

  }
</style>

<h1 id="tutorial-debug-your-kubernetes-apps">Tutorial: Debug Your Kubernetes Apps</h1>

<h4 id="arun-gupta-re-alvarez-parmar-amazon">Arun Gupta &amp; Re Alvarez Parmar, Amazon</h4>

</section><section>

<p>In this presentation we will learn how to troubleshoot Kubernetes applications.</p>

</section><section>

<p>Agenda slide</p>

</section><section>

<p><strong>EKS Architecture</strong></p>

<p><img src="images/eks-arch.jpg" alt="" /></p>

</section><section>

<p><strong>How to setup a Kubernetes cluster</strong></p>

<p><span class='fragment '
  >
   1. Create Master Nodes (aka the Control Plane)
</span></p>

<p><span class='fragment '
  >
   2. use <em>kubectl</em> to connect to the Control Plane
</span></p>

<p><span class='fragment '
  >
   3. Install Worker nodes
</span></p>

<p><span class='fragment '
  >
   4. Deploy apps and add-ons
</span></p>

<p><span class='fragment '
  >
   5. &hellip; profit?
</span></p>
</section>

  

    <section>

<h3 id="eks-architecture">EKS architecture</h3>

<ul>
<li>AWS Managed Control Plane

<ul>
<li>Master nodes</li>
<li>etcd cluster nodes</li>
<li>NLB for API load-balancing</li>
</ul></li>
<li>Highly available</li>
<li>AWS IAM authentication</li>
<li>VPC networking</li>
</ul>

</section><section>

<h3 id="eks-core-tenets">EKS core tenets</h3>

<ul>
<li>Platform for enterprises to run production grade workloads</li>
<li>Provide a native and upstream experience (CNCF Certified)</li>
<li>Provide seamless integration with AWS services</li>
<li>Actively contribute to upstream project</li>
</ul>

</section><section>

<h3 id="kubernetes-components">Kubernetes Components</h3>

<ul>
<li>Master node</li>
<li>Worker Node</li>
<li>kubectl (User)</li>
</ul>

</section><section>

<h3 id="master-node-components">Master node components</h3>

<ul>
<li><strong>apiserver:</strong> exposes APIs for  master nodes</li>
<li><strong>scheduler:</strong> decides which pod should run on which worker node</li>
<li><strong>controller manager:</strong> makes changes attempting to move the current state towards the desired state</li>
<li><strong>etcd:</strong> key/value data store used to store cluster state</li>
</ul>

</section><section>

<h3 id="etcd-design">etcd design</h3>

<ul>
<li>Minimum 3 etcd servers</li>
<li>Spread across availability zones</li>
<li>Uses RAFT protocol</li>
</ul>

</section><section>

<h3 id="worker-node-components">Worker node components</h3>

<ul>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/"><strong>kubelet:</strong></a> handles communication between worker and master nodes</li>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/"><strong>kube-proxy:</strong></a> handles communication between pods, nodes, and the outside world</li>
<li><strong>container runtime:</strong> runs containers on the node.</li>
</ul>

</section><section>

<h3 id="network-considerations">Network considerations</h3>

<ul>
<li>EKS cluster endpoint can be public or private</li>
<li>EKS uses aws-vpc-cni</li>
<li>Worker nodes and Pods get VPC IP</li>
</ul>

</section><section>

<h3 id="aws-vpc-cni-https-github-com-aws-amazon-vpc-cni-k8s"><a href="https://github.com/aws/amazon-vpc-cni-k8s">aws-vpc-cni</a></h3>

<ul>
<li>Pods recieve an IP address from a VPC subnet</li>
<li>Max number of pods is limited by EC2 Instance size</li>
<li>No IP = Pod Pending</li>
<li>Plan for growth</li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-env-vars.html">customize cni variables</a></li>
</ul>

</section><section>

<h1 id="use-cni-metrics-helper-https-docs-aws-amazon-com-eks-latest-userguide-cni-metrics-helper-html"><a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-metrics-helper.html">use CNI Metrics Helper</a></h1>

<p><img src="https://docs.aws.amazon.com/eks/latest/userguide/images/EKS_CNI_metrics.png" alt="" /></p>

</section><section>

<h3 id="kubelet-resource-reservation">Kubelet resource reservation</h3>

<ul>
<li><p>Monitor kubelet on the worker node</p>

<pre><code>journalctl -u kubelet
</code></pre></li>

<li><p>Use kube-reserved to reserve resources for kubelet, container runtime &amp; node problem detector</p>

<pre><code>--kube-reserved=[cpu-100m][,][memory=100Mi][,]
[ephemeral-storage=1Gi][,][pid=1000]
</code></pre></li>

<li><p>Use system-reserved to reserve resources for system daemons liks sshd, udev, kernel</p>

<pre><code>--system-reserved=[cpu-100m][,][memory=100Mi][,]
[ephemeral-storage=1Gi][,][pid=1000]
</code></pre></li>
</ul>

</section><section>

<h3 id="coredns-scaling">Coredns scaling</h3>

<blockquote>
<p>coreDNSMemory required in MB = (Pods + Services)/1000 + 54</p>
</blockquote>

<ul>
<li><p>Scale CoreDNS pods</p>

<pre><code>kubectl -n kube-system scale --current-replicas=2
--replicas=10 deployment/coredns
</code></pre></li>

<li><p>Node-local DNS <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns">addon</a></p>

<ul>
<li>CoreDNS DaemonSet on each node</li>
</ul></li>
</ul>
</section>
    

<section data-noprocess>
 <h2>EKS Architecture</h2>
 <img src="https://d1.awsstatic.com/Getting%20Started/eks-project/EKS-demo-app.e7ce7b188f2662b8573b5881a6b843e09caf729a.png" height=400>
</section>

</section><section>

<section data-markdown>
### How does kubectl work
  <script>
### How does kubectl work

- kubectl communicates with the Kubernetes API server <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="1" --> 
- uses a configuration file generally located at ~/.kube/config <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="2" -->
- In EKS, kubectl + aws-iam-authenticator = ❤️ <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="3" -->
  </script>
</section>

</section><section>

<h3 id="have-you-ever-ran-into-this">&hellip; have you ever ran into this?</h3>

<pre><code>$ kubectl get svc 
error: the server doesn't have a resource type &quot;svc&quot;
</code></pre>

<p><span class='fragment '
  >
  1. Update <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a> &amp; <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">aws-iam-authenticator</a>
</span></p>

<p><span class='fragment '
  >
  2. Update kubeconfig using AWS cli<br />
</span></p>

<p><span class='fragment '
  >
  3. Is cluster endpoint accessible?<br />
</span></p>

</section><section>

<h3 id="check-if-cluster-is-accessible">Check if cluster is accessible</h3>

<pre><code>$ curl -k http://CLUSTER_ENDPOINT/api/v1
</code></pre>

<p>response:</p>

<pre><code>&quot;kind&quot;: &quot;APIResourceList&quot;,
  &quot;groupVersion&quot;: &quot;v1&quot;,
  &quot;resources&quot;: [
  {
        &quot;name&quot;: &quot;bindings&quot;,
              &quot;singularName&quot;: &quot;&quot;,
                    &quot;namespaced&quot;: true,
                          &quot;kind&quot;: &quot;Binding&quot;,
                          &quot;verbs&quot;: [
                                  &quot;create&quot;
...                                        
</code></pre>

</section><section>

<h4 id="use-aws-cli-to-auto-generate-kube-config-file">Use AWS CLI to auto-generate kube config file</h4>

<pre><code>$ aws eks update-kubeconfig --name {cluster-name} 
</code></pre>

</section><section>

<pre><code>$ cat ~/.kube/config

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: {REDACTED} 
    server: https://DFEA886AB17A069545SJDS9F06BCE3DCC.gr7.us-west-2.eks.amazonaws.com
  name: arn:aws:eks:us-west-2:09123456789:cluster/eks1
contexts:
- context:
    cluster: arn:aws:eks:us-west-2:09123456789:cluster/eks1
    user: arn:aws:eks:us-west-2:09123456789:cluster/eks1
  name: arn:aws:eks:us-west-2:09123456789:cluster/eks1
current-context: arn:aws:eks:us-west-2:09123456789:cluster/eks1
kind: Config
preferences: {}
users:
	- name: arn:aws:eks:us-west-2:09123456789:cluster/eks1
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - token
      - -i
      - eks1
      command: aws-iam-authenticator
</code></pre>

</section><section>

<h3 id="aws-auth-config-map">aws-auth config map</h3>

<pre><code>$ kubectl -n kube-system describe configmap aws-auth
-----
Name:         aws-auth
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
mapRoles:
----
- groups:
  - system:bootstrappers
  - system:nodes
  rolearn: arn:aws:iam::09123456789:role/eksctl-eks-nodegroup-ng
  username: system:node:{{EC2PrivateDNSName}}

mapUsers:
----
- userarn: arn:aws:iam::09123456789:user/realvarez
  groups:
    - system:masters

</code></pre>

</section><section>

<h3 id="kubectl-works">kubectl works!</h3>

<pre><code>$ kubectl cluster-info
-----
Kubernetes master is running at https://xxxx.y.region.eks.amazonaws.com
</code></pre>

<p>check cluster status:</p>

<pre><code>$ kubectl get componentstatus 
-----
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}   

</code></pre>
</section>
    <section><section data-shortcode-section>
<p>You created a deployment with 8 replicas:</p>

<pre><code>$ kubectl create -f hello-deployment.yaml 
deployment.apps/hello created
</code></pre>

<div class="fragment" data-fragment-index="1" align=left>
<p>Or scaled an existing deployment to 8 replicas:</p>
</div>

<pre class="fragment" data-fragment-index="1" style="background-color:#f0f0f0" >
$ kubectl scale --replicas=8 deployment hello 
deployment.extensions/hello scaled
</pre>

<p><div class="fragment" data-fragment-index="2">
<p>Deployment shows only 4 replicas are available:</p>
</div>
<pre class="fragment" data-fragment-index="2" style="background-color:#f0f0f0" >
  $ kubectl get deployments
  NAME    READY   UP-TO-DATE   AVAILABLE   AGE
  hello   <sup>4</sup>&frasl;<sub>8</sub>     8            4           23s
</pre></p>

</section><section>

<p><code>get pods</code> shows the same output:</p>

<pre><code>$ kubectl get pods

. . .

=======
NAME                     READY   STATUS    RESTARTS   AGE
hello-6d4fbd5d76-9xqxg   1/1     Running   0          5s
hello-6d4fbd5d76-brv7k   0/1     Pending   0          5s
hello-6d4fbd5d76-hbf8h   0/1     Pending   0          5s
hello-6d4fbd5d76-jdzlw   1/1     Running   0          5s
hello-6d4fbd5d76-jqsfk   0/1     Pending   0          5s
hello-6d4fbd5d76-k29gb   1/1     Running   0          5s
hello-6d4fbd5d76-vjr62   0/1     Pending   0          5s
hello-6d4fbd5d76-z69pp   1/1     Running   0          5s
</code></pre>

</section><section>

<p>Multiple reasons:</p>

<ul>
<li>Not enough resources in the cluster

<ul>
<li>CPU, memory, port</li>
</ul></li>
<li>Node security group does not have an ingress rule with 443 port access</li>
<li>Ensure all nodes are healthy</li>
</ul>

</section><section>

<p>Describe the pod:</p>

<pre><code>$ kubectl describe pod/hello-6d4fbd5d76-brv7k
</code></pre>

<p>Shows the output:</p>

<pre><code>. . .

Events:
  Type     Reason             Age                From                Message
  ----     ------             ----               ----                -------
  Warning  FailedScheduling   42s (x2 over 42s)  default-scheduler   0/4 nodes are available: 4 Insufficient cpu.
</code></pre>

<p>Events are only visible on pods, not on Deployments, ReplicaSet, Job, or any other resource that created pod.</p>

</section><section>

<p>Alternatively, get all events:</p>

<pre><code>$ kubectl get events 
LAST SEEN   TYPE      REASON             KIND   MESSAGE
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
</code></pre>

</section><section>

<p>Or only the warning events:</p>

<pre><code>$ kubectl get events --field-selector type=Warning
</code></pre>

</section><section>

<p>Let&rsquo;s get events only for the pod:</p>

<pre><code>$ kubectl get events --field-selector involvedObject.kind=Pod,involvedObject.name=hello-6d4fbd5d76-brv7k
LAST SEEN   TYPE      REASON             KIND   MESSAGE
4m41s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
</code></pre>

</section><section>

<p>Sort by timestamp:</p>

<pre><code>$ kubectl get events --sort-by='.lastTimestamp'
</code></pre>

</section><section>

<p>Check memory/CPU requirements of pod:</p>

<pre><code>$ kubectl describe deployments/hello
</code></pre>

<p>Output:</p>

<pre><code>  Containers:
   hello:
    Image:      nginx:latest
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Limits:
      cpu:     2
      memory:  2000Mi
    Requests:
      cpu:        2
      memory:     2000Mi
    Environment:  &lt;none&gt;
</code></pre>

</section><section>

<p>Default CPU request is <code>200m</code> and none on memory. There are no limits.</p>

<pre><code>1000m (milicores) = 1 core = 1 CPU = 1 AWS vCPU
</code></pre>

<p>So, that means:</p>

<pre><code>100m cpu = 0.1 cpu
</code></pre>

<p>In this case, CPU request and limits have been specified to <code>2</code> and memory to <code>2GB</code>. So we need 8 blocks of 2 CPU and 2 GB memory.</p>

</section><section>

<p>Check memory/CPU available in cluster:</p>

<pre><code>$ kubectl top nodes
Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)
</code></pre>

</section><section>

<p>Install metrics-server:</p>

<pre><code>$ curl -OL https://github.com/kubernetes-sigs/metrics-server/archive/v0.3.6.tar.gz
&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
$ tar xzvf v0.3.6.tar.gz
=======
tar xzvf v0.3.6.tar.gz
&gt;&gt;&gt;&gt;&gt;&gt;&gt; b4b236b02fdfb0c070be46d1525d19f597d95931
$ kubectl create -f metrics-server-0.3.6/deploy/1.8+/
</code></pre>

</section><section>

<p>Get memory/CPU for nodes:</p>

<pre><code>$ kubectl top nodes
NAME                                           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-192-168-28-108.us-west-2.compute.internal   28m          0%     410Mi           2%        
ip-192-168-48-190.us-west-2.compute.internal   33m          0%     363Mi           2%        
ip-192-168-51-148.us-west-2.compute.internal   29m          0%     338Mi           2%        
ip-192-168-64-166.us-west-2.compute.internal   32m          0%     395Mi           2% 
</code></pre>

</section><section>

<p>Get <em>capacity</em> memory for each node:</p>

<pre><code>$ kubectl get no -o json | jq -r '.items | sort_by(.status.capacity.memory)[]|[.metadata.name,.status.capacity.memory]| @tsv'
ip-192-168-28-108.us-west-2.compute.internal  15950552Ki
ip-192-168-48-190.us-west-2.compute.internal  15950552Ki
ip-192-168-51-148.us-west-2.compute.internal  15950552Ki
ip-192-168-64-166.us-west-2.compute.internal  15950552Ki
</code></pre>

<p>And <em>allocatable</em> memory:</p>

<pre><code>$ kubectl get no -o json | jq -r '.items | sort_by(.status.allocatable.memory)[]|[.metadata.name,.status.allocatable.memory]| @tsv'
ip-192-168-28-108.us-west-2.compute.internal  15848152Ki
ip-192-168-48-190.us-west-2.compute.internal  15848152Ki
ip-192-168-51-148.us-west-2.compute.internal  15848152Ki
ip-192-168-64-166.us-west-2.compute.internal  15848152Ki
</code></pre>

</section><section>

<p>How is <strong>allocatable</strong> calculated?</p>

<pre><code>[Allocatable] = [Node Capacity] - [Kube-Reserved] - [System-Reserved] - [Hard-Eviction-Threshold]
</code></pre>

<p><span class='fragment '
  >
  Explained at <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md</a>.
</span></p>

</section><section>

<p>And do the same for capacity CPU:</p>

<pre><code>$ kubectl get no -o json | jq -r '.items | sort_by(.status.capacity.cpu)[]|[.metadata.name,.status.capacity.cpu]| @tsv'
ip-192-168-28-108.us-west-2.compute.internal  4
ip-192-168-48-190.us-west-2.compute.internal  4
ip-192-168-51-148.us-west-2.compute.internal  4
ip-192-168-64-166.us-west-2.compute.internal  4
</code></pre>

<p>And allocatable CPU:</p>

<pre><code>$ kubectl get no -o json | jq -r '.items | sort_by(.status.allocatable.cpu)[]|[.metadata.name,.status.allocatable.cpu]| @tsv'
ip-192-168-28-108.us-west-2.compute.internal  4
ip-192-168-48-190.us-west-2.compute.internal  4
ip-192-168-51-148.us-west-2.compute.internal  4
ip-192-168-64-166.us-west-2.compute.internal  4
</code></pre>

<p>So, there is enough memory and CPU. Why the pods are not getting scheduled?</p>

</section><section>

<p>EKS AMI now sets a minimum <code>evictionHard</code> and <code>kubeReserved</code> values: <a href="https://github.com/awslabs/amazon-eks-ami/pull/350">https://github.com/awslabs/amazon-eks-ami/pull/350</a>.</p>

<p>Alternatively, you can set these values using eksctl <a href="https://eksctl.io/usage/customizing-the-kubelet/">https://eksctl.io/usage/customizing-the-kubelet/</a>.</p>

</section><section>

<p>Cluster autoscaler serves two purpose:</p>

<ul>
<li>Pods fail to run due to insufficient resources</li>
<li>Recycle nodes that are underutilized for an extended period of time</li>
</ul>

<p>Let&rsquo;s install it!</p>

</section><section>

<p>Create IAM policy with autoscaling permissions and attach to the worker node IAM roles.</p>

<p>Create IAM policy:</p>

<pre><code>$ aws iam create-policy --policy-name AmazonEKSAutoscalingPolicy --policy-document file://../../resources/manifests/autoscaling-policy.json
{
    &quot;Policy&quot;: {
        &quot;PolicyName&quot;: &quot;AmazonEKSAutoscalingPolicy&quot;,
        &quot;PolicyId&quot;: &quot;ANPARKOFJSCVVWD4MQEKB&quot;,
        &quot;Arn&quot;: &quot;arniam:policy/AmazonEKSAutoscalingPolicy&quot;,
        . . .
    }
}
</code></pre>

<p>Attach policy to the IAM role:</p>

<pre><code>$ ROLE_NAME=$(aws iam list-roles \
  --query \
  'Roles[?contains(RoleName,`debug-k8s-nodegroup`)].RoleName' --output text)
$ aws iam attach-role-policy \
  --role-name $ROLE_NAME \
  --policy-arn arniam:policy/AmazonEKSAutoscalingPolicy
</code></pre>

</section><section>

<p>Setup auto discovery of Auto Scaling Groups by Cluster Autoscaler by attaching tags to the nodegroup:</p>

<pre><code>$ ASG_NAME=$(aws autoscaling describe-auto-scaling-groups \
  --query \
  'AutoScalingGroups[?contains(AutoScalingGroupName,`debug-k8s-nodegroup`)].AutoScalingGroupName' --output text)
$ aws autoscaling create-or-update-tags \
  --tags \
  ResourceId=$ASG_NAME,ResourceType=auto-scaling-group,Key=k8s.io/cluster-autoscaler/enabled,Value=something,PropagateAtLaunch=true \
  ResourceId=$ASG_NAME,ResourceType=auto-scaling-group,Key=k8s.io/cluster-autoscaler/debug-k8s,Value=something,PropagateAtLaunch=true
</code></pre>

</section><section>

<p>Now, create Cluster Autoscaler:</p>

<pre><code>$ CA_FILE=cluster-autoscaler-autodiscover.yaml
$ curl -o ${CA_FILE} https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml
$ sed -i -e 's/&lt;YOUR CLUSTER NAME&gt;/debug-k8s/' ${CA_FILE}
$ kubectl create -f ${CA_FILE}
</code></pre>

</section><section>

<p>Check Cluster Autoscaler logs:</p>

<pre><code>$ kubectl logs -f deployment/cluster-autoscaler -n kube-system
</code></pre>

<p>Shows the output:</p>

<pre><code>I1113 0100.754350       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-hbf8h is unschedulable
I1113 0100.754358       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-vjr62 is unschedulable
I1113 0100.754365       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-brv7k is unschedulable
I1113 0100.754371       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-jqsfk is unschedulable
I1113 0100.754407       1 scale_up.go:300] Upcoming 0 nodes
I1113 0100.754416       1 scale_up.go:335] Skipping node group eksctl-debug-k8s-nodegroup-ng-bb0efd30-NodeGroup-H77X21MZFGGH - max size reached
I1113 0100.754426       1 scale_up.go:411] No expansion options
</code></pre>

</section><section>

<p>Update Autoscaling Group limits:</p>

<pre><code>$ aws autoscaling update-auto-scaling-group --auto-scaling-group-name $ASG_NAME --max-size 8
</code></pre>

<p>Cluster Autoscaler logs are updated:</p>

<pre><code>I1113 0111.009046       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-jqsfk is unschedulable
I1113 0111.009052       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-hbf8h is unschedulable
I1113 0111.009057       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-vjr62 is unschedulable
I1113 0111.009062       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-brv7k is unschedulable
I1113 0111.009098       1 scale_up.go:300] Upcoming 0 nodes
I1113 0111.009322       1 waste.go:57] Expanding Node Group eksctl-debug-k8s-nodegroup-ng-bb0efd30-NodeGroup-H77X21MZFGGH would waste 50.00% CPU, 93.58% Memory, 71.79% Blended
I1113 0111.009346       1 scale_up.go:418] Best option to resize: eksctl-debug-k8s-nodegroup-ng-bb0efd30-NodeGroup-H77X21MZFGGH
I1113 0111.009356       1 scale_up.go:422] Estimated 4 nodes needed in eksctl-debug-k8s-nodegroup-ng-bb0efd30-NodeGroup-H77X21MZFGGH
I1113 0111.009374       1 scale_up.go:501] Final scale-up plan: [{eksctl-debug-k8s-nodegroup-ng-bb0efd30-NodeGroup-H77X21MZFGGH 4-&gt;8 (max: 8)}]
</code></pre>

</section><section>

<p>Lets check the pods again:</p>

<pre><code>$ kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
hello-6d4fbd5d76-9xqxg   1/1     Running   0          6m30s
hello-6d4fbd5d76-brv7k   1/1     Running   0          6m30s
hello-6d4fbd5d76-hbf8h   1/1     Running   0          6m30s
hello-6d4fbd5d76-jdzlw   1/1     Running   0          6m30s
hello-6d4fbd5d76-jqsfk   1/1     Running   0          6m30s
hello-6d4fbd5d76-k29gb   1/1     Running   0          6m30s
hello-6d4fbd5d76-vjr62   1/1     Running   0          6m30s
hello-6d4fbd5d76-z69pp   1/1     Running   0          6m30s
</code></pre>

</section><section>

<p>Another similar use case:</p>

<pre><code>$ kubectl get pods -l app=mnist,type=inference
NAME                             READY   STATUS    RESTARTS   AGE
mnist-inference-cd78cfd5-hcvfd   0/1     Pending   0          3m48s
</code></pre>

</section><section>

<p>Get details about the pod:</p>

<pre><code>$ kubectl describe pod mnist-inference-cd78cfd5-hcvfd

. . .

Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  3s (x8 over 5m32s)  default-scheduler  0/2 nodes are available: 2 Insufficient nvidia.com/gpu.
</code></pre>

</section><section>

<p>Need to create a cluster with more GPUs.</p>

</section><section>

<p>Let&rsquo;s look at a different part of the problem now.</p>

<p>Pods are all scheduled but not able to meet throughput and/or latency needs.</p>

</section><section>

<p>Create horizontal pod autoscaler</p>

<p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a> (HPA) scales the pods in a deployment or replica set. It is implemented as a K8s API resource and a controller. The controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. It obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).</p>

<p>Default HPA loop is 15 seconds (controlled by <code>--horizontal-pod-autoscaler-sync-period</code>)</p>

<pre><code></code></pre>

</section>
</section>
    <section>

<h1 id="what-about-stateful-containers">What about stateful containers?</h1>

</section><section>

<h3 id="pods-are-ephemeral-by-nature">Pods are ephemeral by nature</h3>

<ul>
<li><p>Volumes allow pods to persist data</p></li>

<li><p>Volumes are accessible to all containers in a pod</p></li>

<li><p>Data can persist even after pod termination</p></li>
</ul>

</section><section>

<h3 id="persistentvolume-pv">PersistentVolume (PV)</h3>

<blockquote>
<p>pre-provisioned storage in the cluster or dynamically provisioned by storage class</p>
</blockquote>

<h3 id="persistentvolumeclaim-pvc">PersistentVolumeClaim (PVC)</h3>

<blockquote>
<p>request for storage by a user</p>
</blockquote>

<h3 id="storageclass">StorageClass</h3>

<blockquote>
<p>administrator provided &ldquo;classes&rdquo; of storage</p>
</blockquote>

<aside class="notes">
Persistent Volumes provide a plugin model for storage in Kubernetes. How storage is provided is abstracted from how it is consumed

</aside>

</section><section>

<h3 id="what-are-statefulsets">What are statefulsets?</h3>

<ul>
<li>Provide pods with storage to persist data</li>
<li>Create PVC dynamically using <code>volumeClaimTemplates</code></li>
<li>Each pod gets its own dedicated PVC</li>
<li>Create headless service type (<code>clusterIP: None</code>)</li>
<li>Ordered deployment and scaling
<aside class="notes">
“As you probably realize, we said PVCs are created and associated with the Pods, but we didn’t say anything about PVs. That is because StatefulSets do not manage PVs in any way. The storage for the Pods must be provisioned in advance by an admin, or provisioned on-demand by a PV provisioner based on the requested storage class and ready for consumption by the stateful Pods.”
<aside class="notes"></section><section></li>
</ul>

<h3 id="step-1-create-storage-class">Step 1. Create storage class</h3>

<pre><code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: mysql-gp2
  provisioner: kubernetes.io/aws-ebs
  parameters:
    type: gp2
    reclaimPolicy: Delete
    mountOptions:
    - debug
</code></pre>

</section><section>

<h3 id="step-2-use-volumeclaimtemplates-in-the-statefulset">Step 2. use volumeClaimTemplates in the Statefulset</h3>

<pre><code>volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: mysql-gp2
      resources:
        requests:
          storage: 10Gi
</code></pre>

</section><section>

<h3 id="mysql-statefulsets-on-kubernetes">mysql statefulsets on Kubernetes</h3>

<pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  spec:
    selector:
      matchLabels:
        app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
   ...

        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql

  ...
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: mysql-gp2
      resources:
        requests:
          storage: 10Gi
</code></pre>

<p>See full yaml <a href="https://eksworkshop.com/statefulset/statefulset.files/mysql-statefulset.yml">here</a></p>

</section><section>

<h3 id="persistent-storage-options-in-eks">Persistent storage options in EKS</h3>

<ul>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html">EBS CSI Driver</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html">EFS CSI Driver</a></li>
<li><a href="https://github.com/kubernetes-sigs/aws-fsx-csi-driver">FSx for Lustre CSI Driver (Alpha)</a></li>
</ul>

</section><section>

<h3 id="eks-ebs">EKS + EBS</h3>

<p><ul>
<li>Persistent volumes can be <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/dynamic-provisioning">dynamically</a> or
<a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/static-provisioning">statically</a> provisioned</li>
<li>Volumes can be <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/resizing">resized</a> and support <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/snapshot">Snapshots</a></li>
<li>Volumes are local to AZ, use EFS where possible</li>
</ul>
</aside></aside></p>
</section>
    <section>

<h1 id="going-public">Going public</h1>

</section><section>

<h3 id="options-for-exposing-kubernetes-apps">Options for exposing Kubernetes apps</h3>

<ul>
<li><p>ServiceType=LoadBalancer. Uses Classic load balancer.</p>

<ul>
<li><p>Use Network load balancer, annotate your service:</p>

<pre><code>service.beta.kubernetes.io/aws-load-balancer-type: nlb
</code></pre></li>
</ul></li>

<li><p>Ingress using <a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller">alb-ingresss-controller</a></p>

<ul>
<li><p>For internal only load balancer, annotate your service:</p>

<pre><code>service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0

</code></pre></li>
</ul></li>

<li><p><a href="https://github.com/kubernetes/ingress-nginx">NGINX Ingress Controller</a></p></li>
</ul>

</section><section>

<h3 id="if-load-balancer-fails-to-deploy">If load balancer fails to deploy</h3>

<ul>
<li>Check <a href="https://docs.aws.amazon.com/en_pv/eks/latest/userguide/network_reqs.html">tags</a></li>

<li><p>Check service</p>

<pre><code>kubectl describe service
</code></pre></li>

<li><p>Check <a href="https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html">EKS IAM Service Role</a></p></li>
</ul>

</section><section>

<h3 id="troubleshoot-latency-in-app">Troubleshoot latency in app</h3>

<ul>
<li>Check <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html">CloudWatch Metrics for load balancer</a></li>

<li><p>Check latency within cluster</p>

<pre><code>kubectl apply -f https://k8s.io/examples/application/shell-demo.yaml

kubectl exec -it shell-demo -- /bin/bash

root@shell-demo:/# apt-get update 
root@shell-demo:/# apt-get install curl
root@shell-demo:/# curl http://10.100.2.39 &lt;-- IP of the service
</code></pre></li>

<li><p>Implement <a href="https://docs.aws.amazon.com/whitepapers/latest/microservices-on-aws/distributed-monitoring.html">tracing</a>
preferably with a <a href="https://aws.amazon.com/app-mesh/">Service Mesh</a></p></li>
</ul>
</section>
    <section>

<h1 id="how-to-find-problems">How to find problems?</h1>

</section><section>

<h3 id="monitoring-tools">🎛 Monitoring tools</h3>

<ul>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/prometheus.html">Prometheus</a> + <a href="https://eksworkshop.com/monitoring/">Grafana</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/deploy-container-insights-EKS.html">CloudWatch Container Insights</a></li>
<li>Kubernetes <a href="https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html">Dashboard</a></li>
<li>Commercial solutions (e.g., Datadog, Dynatrace, Sysdig, etc.)</li>
</ul>

<aside class="notes">
Prometheus provides metrics storage. Grafana provides dashboards. 
</aside>

</section><section>

<section data-markdown>
### Key things to monitor
  <script> 
  ### Key things to monitor
- Nodes <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="1" -->
   - CPU/memory usage, disk pressure & network <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="1" -->
- Pods, deployments and services <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="2" -->
   - Pod count <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="2" -->
   - Container health checks <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="2" -->
- coreDNS <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="3" -->
   - DNS latency <!-- .element: class="fragment fade-in-then-semi-out" data-fragment-index="3" -->
  </script>
</section>

</section><section>

<p>Install Prometheus</p>

<pre><code>$ kubectl create namespace prometheus
$ helm install stable/prometheus \
    --name prometheus \
    --namespace prometheus \
    --set alertmanager.persistentVolume.storageClass=&quot;gp2&quot; \
    --set server.persistentVolume.storageClass=&quot;gp2&quot;
</code></pre>

<p>verify Prometheus</p>

<pre><code>$ kubectl -n prometheus get pods
prometheus-alertmanager-58c6876d4c-6p5qs              2/2     Running   0          20s
prometheus-kube-state-metrics-74b57df8bd-jksg8        1/1     Running   0          20s
prometheus-node-exporter-2nvkk                        1/1     Running   0          20s
prometheus-node-exporter-fzlts                        1/1     Running   0          20s
prometheus-node-exporter-s4nsc                        1/1     Running   0          20s
prometheus-pushgateway-c497ff84d-bxm5b                1/1     Running   0          20s
prometheus-server-796c867465-q775s                    2/2     Running   0          20s
</code></pre>

</section><section>

<p>access Prometheus</p>

<pre><code>$ kubectl port-forward -n prometheus deploy/prometheus-server 8080:9090
</code></pre>

<p><center>
<img src="https://eksworkshop.com/images/prometheus-targets.png" height="300" >
</center></p>

</section><section>

<p>Install Grafana</p>

<pre><code>$ kubectl create namespace grafana
$ helm install stable/grafana \
    --name grafana \
    --namespace grafana \
    --set persistence.storageClassName=&quot;gp2&quot; \
    --set adminPassword='EKS!sAWSome' \
    --set datasources.&quot;datasources\.yaml&quot;.apiVersion=1 \
    --set datasources.&quot;datasources\.yaml&quot;.datasources[0].name=Prometheus \
    --set datasources.&quot;datasources\.yaml&quot;.datasources[0].type=prometheus \
    --set datasources.&quot;datasources\.yaml&quot;.datasources[0].url=http://prometheus-server.prometheus.svc.cluster.local \
    --set datasources.&quot;datasources\.yaml&quot;.datasources[0].access=proxy \
    --set datasources.&quot;datasources\.yaml&quot;.datasources[0].isDefault=true \
    --set service.type=LoadBalancer
</code></pre>

<p>verify Grafana pods</p>

<pre><code>$ kubectl -n grafana get pods
NAME                          READY     STATUS    RESTARTS   AGE
pod/grafana-b9697f8b5-t9w4j   1/1       Running   0          2m
</code></pre>

</section><section>

<p>Obtain Grafana ELB URL</p>

<pre><code>$ export ELB=$(kubectl get svc \
    -n grafana grafana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
$ echo &quot;http://$ELB&quot;
</code></pre>

<p>Get password for admin user</p>

<pre><code>$ kubectl get secret --namespace grafana grafana \
      -o jsonpath=&quot;{.data.admin-password}&quot; | base64 --decode ; echo
</code></pre>

</section><section>

<h2 id="community-created-dashboard-3131">Community created Dashboard #3131</h2>

<p><center>
  <img src="https://eksworkshop.com/images/grafana-all-nodes.png" height="75%" width="75%">
</center></p>

</section><section>

<h2 id="community-created-dashboard-3146">Community created Dashboard #3146</h2>

<p><center>
  <img src="https://eksworkshop.com/images/grafana-all-pods.png" height="75%" width="75%">
</center></p>

</section><section>

<h3 id="cloudwatch-container-inights">CloudWatch Container Inights</h3>

<ul>
<li>Collect, aggregate, and summarize metrics</li>
<li>Capture logs (uses FluentD)</li>
<li>Get diagnostic information, such as container restart failures</li>
</ul>

</section><section>

<h3 id="cloudwatch-container-insights-components">Cloudwatch Container Insights components</h3>

<ol>
<li>CloudWatch agent daemonset</li>

<li><p>FluentD daemonset</p>

<pre><code>$ kubectl get ds -n amazon-cloudwatch
NAME                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
cloudwatch-agent     3         3         3       3            3           &lt;none&gt;          21h
fluentd-cloudwatch   3         3         3       3            3           &lt;none&gt;          21h
</code></pre></li>
</ol>

</section><section>

<h4 id="cloudwatch-container-insights-dashboard">CloudWatch Container Insights Dashboard</h4>

<p><center><img src=images/Container-insights.png height="75%" width="75%"></center></p>

</section><section>

<p><img src="images/Container-insights-pod.png" alt="" /></p>

</section><section>

<h3 id="container-monitoring-options">Container monitoring options</h3>

<ul>
<li>Readiness probe</li>
<li>Liveness probe</li>
<li>Tracing (OpenTracing, AWS X-ray)</li>
<li>Process health</li>
<li>Process metrics</li>
<li>Process logs</li>
</ul>

<aside class="notes">
A good cloud native container must provide APIs for runtime to monitor the health, if checks fail, an action should be triggered.
</aside>
</section>
    <section><p>Thank you!</p>
</section>

</div>
      
    </div>
<script type="text/javascript" src=/debug-k8s-apps/reveal-hugo/object-assign.js></script>

<a href="/debug-k8s-apps/reveal-js/css/print/" id="print-location" style="display: none;"></a>
<script type="text/javascript">
  var printLocationElement = document.getElementById('print-location');
  var link = document.createElement('link');
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = printLocationElement.href + (window.location.search.match(/print-pdf/gi) ? 'pdf.css' : 'paper.css');
  document.getElementsByTagName('head')[0].appendChild(link);
</script>

<script type="application/json" id="reveal-hugo-site-params">{"controls":false,"custom_theme":"stylesheets/robot-lung.css","custom_theme_compile":true,"slide_number":true}</script>
<script type="application/json" id="reveal-hugo-page-params">{"controls":false,"hash":false,"height":"80%","margin":0.1,"progress":false,"slide_number":true,"theme":"white","transition":"fade","width":"80%"}</script>

<script src="/debug-k8s-apps/reveal-js/js/reveal.js"></script>

<script type="text/javascript">
  
  
  function camelize(map) {
    if (map) {
      Object.keys(map).forEach(function(k) {
        newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
        if (newK != k) {
          map[newK] = map[k];
          delete map[k];
        }
      });
    }
    return map;
  }
  
  var revealHugoDefaults = { center: true, controls: true, history: true, progress: true, transition: "slide" };
  var revealHugoSiteParams = JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);
  var revealHugoPageParams = JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);
  
  var options = Object.assign({},
    camelize(revealHugoDefaults),
    camelize(revealHugoSiteParams),
    camelize(revealHugoPageParams));
  Reveal.initialize(options);
</script>


  
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/markdown/marked.js"></script>
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/markdown/markdown.js"></script>
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/highlight/highlight.js"></script>
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/zoom-js/zoom.js"></script>
  
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/notes/notes.js"></script>



    <style>
  #logo {
    position: absolute;
    top: 10px;
    left: 10px;
    width: 10%;
  }
</style>

<img id="logo" src="images/aws-logo.svg" alt="AWS Smile!">

    
  </body>
</html>
