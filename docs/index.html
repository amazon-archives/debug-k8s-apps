<!doctype html>
<html lang="en">
  <head>
	<meta name="generator" content="Hugo 0.58.3" />
    <meta charset="utf-8">
<title>Tutorial: Debug Your Kubernetes Apps - Arun Gupta &amp; Re Alvarez Parmar, Amazon Web Services</title>


<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="/debug-k8s-apps/reveal-js/css/reset.css">
<link rel="stylesheet" href="/debug-k8s-apps/reveal-js/css/reveal.css"><link rel="stylesheet" href="/debug-k8s-apps/reveal-js/css/theme/white.css" id="theme">
<link rel="stylesheet" href="/debug-k8s-apps/highlight-js/default.min.css">
    <link rel="shortcut icon" href="images/favicon.ico" >

  </head>
  <body>
    <div class="reveal">
      <div class="slides">
  

    <section>

<style type="text/css">
  .reveal {
    font-size: 30px;
  }
  .reveal p {
    text-align: left;
    font-size: 25px;
  }
  .reveal h3 {
    text-align: left;
  }
  .reveal ul {
    display: block;
    font-size: 25px;
  }
  .reveal ol {
    display: block;
    font-size: 25px;
  }
  .reveal code {
   font-size: 15px;
  } 
  .reveal pre code {
   font-size: 15px;
  }
  .reveal section img {
    border-style: none;
    box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
    max-height: 65%;
    max-width: auto;
    margin-left: auto;
    margin-right: auto;
    tex-align: center;
    display: block;
  }
</style>

<section data-background-image="images/kubecon-slide-theme.png"
data-background-size=cover data-background-color="#000000">
</section>

</section><section>

<section data-background-image="images/kubecon-slide-title.png"
data-background-size=contain data-background-color="#000000" >
</section>

</section><section>

<p><center>
<img src="images/dilbert-k8s.jpeg"/>
</center></p>

</section><section>

<p><center>
<img src="images/k8s-heisenberg.png"/>
</center></p>

<aside class="notes">
In this presentation we will learn how to troubleshoot Kubernetes applications. 
</aside>

</section><section>

<h3 id="topics">Topics</h3>

<ol>
<li><p>Cluster Design</p></li>

<li><p>Networking</p></li>

<li><p>Kubectl</p></li>

<li><p>Pods</p></li>

<li><p>Resource Reservation</p></li>

<li><p>StatefulSets</p></li>

<li><p>Load Balancing &amp; Ingress</p></li>

<li><p>Monitoring</p></li>
</ol>
</section>

  

    <section><section data-shortcode-section>
<h2 id="section-1">Section 1:</h2>

<h2 id="cluster-design">Cluster Design</h2>

</section><section>

<h3 id="kubernetes-components">Kubernetes Components</h3>

<h4 id="img-src-images-k8s-cluster-1-png-height-65-width-65"><img src="images/k8s-cluster-1.png" height="65%" width="65%" /></h4>

</section><section>

<h3 id="how-to-setup-a-kubernetes-cluster">How to setup a Kubernetes cluster</h3>

<p><span class='fragment '
  >
  1. Create Controller nodes (aka the <em>Control Plane</em>)
</span></p>

<p><span class='fragment '
  >
  2. Setup <em>etcd</em> and connect to Controller
</span></p>

<p><span class='fragment '
  >
  3. Use <em>kubectl</em> to connect to the Control Plan
</span></p>

<p><span class='fragment '
  >
  4. Install Worker nodes (aka the <em>Data Plane</em>)
</span></p>

<p><span class='fragment '
  >
  5. Deploy apps and add-ons
</span></p>

<p><span class='fragment '
  >
  6. &hellip; profit?
</span></p>

</section><section>

<h3 id="design-considerations-for-k8s-cluster">Design considerations for k8s cluster</h3>

<ul>
<li>Control Plane: hosted or self-managed</li>
<li>Data Plane: hosed or self-managed</li>
<li>EC2 instance size based upon number of worker nodes</li>
<li>Operating system?</li>
<li>How many pods per cluster?</li>
<li>Etcd co-located with master?</li>
<li>Secure and backup etcd, how often?</li>
<li>High availability</li>
<li>Disaster recovery</li>
<li>Upgrade k8s versions</li>
<li>Staying up to date with k8s release cadence, CVEs and security patches</li>
<li>Monitoring and logging</li>
<li>One big cluster, multiple small clusters</li>
<li>Blast radius</li>
</ul>

</section><section>

<h3 id="amazon-eks-architecture">Amazon EKS Architecture</h3>

<h4 id="img-src-images-k8s-cluster-2-png-width-65-height-65"><img src="images/k8s-cluster-2.png" width="65%", height="65%"/></h4>

</section><section>

<h3 id="amazon-eks-architecture-1">Amazon EKS architecture</h3>

<ul>
<li>AWS Managed Control Plane

<ul>
<li>Master nodes</li>
<li>etcd cluster nodes</li>
<li>NLB for API load-balancing</li>
</ul></li>
<li>Highly available</li>
<li>AWS IAM authentication</li>
<li>VPC networking</li>
</ul>

</section><section>

<!--

### Amazon EKS core tenets

- Platform for enterprises to run production grade workloads
- Provide a native and upstream experience (CNCF Certified)
- Provide seamless integration with AWS services
- Actively contribute to upstream project

---

--->

<h3 id="kubectl">Kubectl</h3>

<p>One CLI to control your k8s cluster</p>

<p><img src="images/k8s-cluster-3.png" height="65%" width="65%"></p>

</section><section>

<h3 id="master-node-components">Master node components</h3>

<ul>
<li><strong>API-Server:</strong> exposes APIs for  master nodes</li>
<li><strong>Controller Manager:</strong> makes changes attempting to move the current state towards the desired state</li>
<li><strong>Scheduler:</strong> decides which pod should run on which worker node</li>
<li><strong>etcd:</strong> key/value data store used to store cluster state</li>
</ul>

</section><section>

<h3 id="etcd-design">etcd design</h3>

<ul>
<li>Minimum 3 etcd servers</li>
<li>Spread across availability zones</li>
<li>Uses RAFT protocol</li>
</ul>

</section><section>

<h3 id="worker-node-components">Worker node components</h3>

<ul>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/"><strong>kubelet</strong></a>: handles communication between worker and master nodes</li>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/"><strong>kube-proxy</strong></a>: handles communication between pods, nodes, and the outside world</li>
<li><strong>container runtime</strong> (CRI): runs containers on the node.</li>
</ul>

</section><section>

<h3 id="create-eks-cluster">Create EKS cluster</h3>

<pre><code>eksctl create cluster -f resources/manifests/eks-cluster.yaml
</code></pre>

</section><section>

<h3 id="eksctl-configuration-file">eksctl configuration file</h3>

<pre><code>apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: debug-k8s
  region: us-east-1

nodeGroups:
  - name: nodegroup
    instanceType: m5.xlarge
    desiredCapacity: 4
    ssh:
      allow: true
      publicKeyName: arun-us-east1

cloudWatch:
  clusterLogging:
    # enable specific types of cluster control plane logs
    enableTypes: [&quot;audit&quot;, &quot;authenticator&quot;, &quot;controllerManager&quot;]
    # all supported types: &quot;api&quot;, &quot;audit&quot;, &quot;authenticator&quot;, &quot;controllerManager&quot;, &quot;scheduler&quot;
    # supported special values: &quot;*&quot; and &quot;all&quot;
</code></pre>

</section>
</section>
    <section><section data-shortcode-section>
<h2 id="section-2">Section 2:</h2>

<h2 id="networking">Networking</h2>

</section><section>

<h3 id="networking-1">Networking</h3>

<ul>
<li>Container Networking Interface (CNI)</li>
<li>CoreDNS (DNS Server)</li>
<li>Amazon EKS Endpoint access</li>
</ul>

</section><section>

<h2 id="section-2-1">Section 2.1:</h2>

<h2 id="networking-2">Networking</h2>

<h2 id="container-networking-interface-cni">Container Networking Interface (CNI)</h2>

</section><section>

<h3 id="container-networking-interface">Container Networking Interface</h3>

<p><a href="https://github.com/containernetworking/cni">Container Networking Interface</a> consists of a <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">specification</a>, libraries for writing plugins to configure network interface in Linux containers, and a number of supported plugins.</p>

<ul>
<li>EKS uses <a href="https://github.com/aws/amazon-vpc-cni-k8s">amazon-vpc-cni</a></li>
<li>Worker nodes and pods get IP address from VPC</li>
</ul>

</section><section>

<h3 id="amazon-vpc-cni-for-kubernetes">Amazon VPC CNI for Kubernetes</h3>

<ul>
<li>Elastic Network Interface (ENI) attached to the worker node</li>
<li>Pods recieve an IP address from a VPC subnet</li>
<li>Max number of pods is limited by ENIs that can be attached to EC2 instance

<ul>
<li>defined at <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a></li>
</ul></li>
<li>No IP = Pod Pending</li>
<li>Plan for growth</li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-env-vars.html">Customize cni variables</a></li>
</ul>

</section><section>

<p>Create a deployment:</p>

<pre><code>$ kubectl create -f resources/manifests/hello-deployment.yaml 
</code></pre>

</section><section>

<p>Scale to 240 replicas</p>

<pre><code>$ kubectl scale --replicas=240 deployment hello
</code></pre>

</section><section>

<p>Get deployments:</p>

<pre><code>$ kubectl get deployments
NAME    READY     UP-TO-DATE   AVAILABLE   AGE
hello   222/240   240          222         5m27s
</code></pre>

</section><section>

<p>Get pending pods:</p>

<pre><code>$ kubectl get pods --field-selector=status.phase==Pending
NAME                    READY   STATUS    RESTARTS   AGE
hello-9fdd9558f-2x8d8   0/1     Pending   0          3m55s
hello-9fdd9558f-4s4hl   0/1     Pending   0          3m55s
hello-9fdd9558f-5fsfv   0/1     Pending   0          3m55s
hello-9fdd9558f-5jffb   0/1     Pending   0          3m55s
hello-9fdd9558f-69f6x   0/1     Pending   0          3m54s
hello-9fdd9558f-6pfzb   0/1     Pending   0          3m55s
hello-9fdd9558f-7l844   0/1     Pending   0          3m55s
hello-9fdd9558f-8zmhk   0/1     Pending   0          3m55s
hello-9fdd9558f-d48ng   0/1     Pending   0          3m55s
hello-9fdd9558f-hqpwp   0/1     Pending   0          3m55s
hello-9fdd9558f-jjs7b   0/1     Pending   0          3m55s
hello-9fdd9558f-jsqsv   0/1     Pending   0          3m55s
hello-9fdd9558f-kjjt4   0/1     Pending   0          3m55s
hello-9fdd9558f-m7fnc   0/1     Pending   0          3m55s
hello-9fdd9558f-pp9ls   0/1     Pending   0          3m55s
hello-9fdd9558f-sqnvg   0/1     Pending   0          3m55s
hello-9fdd9558f-v4m4z   0/1     Pending   0          3m54s
hello-9fdd9558f-vcsx6   0/1     Pending   0          3m55s
</code></pre>

</section><section>

<p>Get events:</p>

<pre><code>$ kubectl get events --field-selector involvedObject.kind=Pod,type=Warning
</code></pre>

</section><section>

<p>Shows the output:</p>

<pre><code>11m         Warning   FailedCreatePodSandBox   pod/hello-9fdd9558f-zns6z   Failed create pod sandbox: rpc error: 
                                                                           code = Unknown desc = failed to set 
                                                                           up sandbox container &quot;2f43374edb1fdc76
                                                                           74587fab2f040bdc8e29abb4cf0ac7a12daea4
                                                                           f04bab8fe4&quot; network for pod 
                                                                           &quot;hello-9fdd9558f-zns6z&quot;: 
                                                                           NetworkPlugin cni failed to set up 
                                                                           pod &quot;hello-9fdd9558f-zns6z_default&quot; 
                                                                           network: add cmd: failed to assign an 
                                                                           IP address to container
</code></pre>

</section><section>

<h3 id="ip-address-allocation">IP address allocation</h3>

<p>ipamD (IP Address Management Daemon) allocates ENIs and secondary IP addresses from the instance subnet.</p>

<p>Each ENI uses 1 IP address to attach to the instance.</p>

<p>With <strong>N</strong> ENIs and <strong>M</strong> addresses:</p>

<pre><code>Maximum number of IPs = min((N * (M - 1)), free IPs in the subnet)
</code></pre>

</section><section>

<p>For our cluster with <strong>m5.xlarge</strong> node type:</p>

<pre><code>N = 15
M = 4
</code></pre>

<p><span class='fragment '
  >
  Default subnet is <strong>192.168.0.0/19</strong> =&gt; 8192 IPs
</span></p>

<p><span class='fragment '
  >
  Maximum number of IP addresses per host is <strong>56 = min(4 * (15 - 1), 8192)</strong>
</span></p>

<p><span class='fragment '
  >
  <strong>v1.16</strong> <a href="https://kubernetes.io/docs/setup/best-practices/cluster-large/">recommends</a> no more than 100 pods per node
</span></p>

</section><section>

<h3 id="cni-metrics-helper">CNI Metrics Helper</h3>

<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-metrics-helper.html">CNI Metrics Helper</a> helps you track how many IP addresses have been assigned and how many are available.</p>

<p>The following metrics are collected for your cluster and exported to CloudWatch:</p>

<ul>
<li>Maximum number of ENIs that the cluster can support</li>
<li>Number of ENIs have been allocated to pods</li>
<li>Number of IP addresses currently assigned to pods</li>
<li>Total and maximum numbers of IP addresses available</li>
<li>Number of ipamD errors</li>
</ul>

</section><section>

<h3 id="create-cni-metrics-helper-policy">Create CNI Metrics Helper Policy</h3>

<pre><code>aws iam create-policy \
  --policy-name CNIMetricsHelperPolicy \
  --description &quot;Grants permission to write metrics to CloudWatch&quot; \
  --policy-document file://./resources/manifests/cni-metrics-policy.json
</code></pre>

</section><section>

<p>Attach policy to the worker nodes IAM role:</p>

<pre><code>$ ROLE_NAME=$(aws iam list-roles \
  --query \
  'Roles[?contains(RoleName,`debug-k8s-nodegroup`)].RoleName' --output text)
aws iam attach-role-policy \
  --role-name $ROLE_NAME \
  --policy-arn arniam:policy/CNIMetricsHelperPolicy
</code></pre>

</section><section>

<h3 id="deploy-cni-metrics-helper">Deploy CNI metrics helper</h3>

<pre><code>$ kubectl apply -f \
    https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.5/config/v1.5/cni-metrics-helper.yaml
...
$ kubectl get deployment cni-metrics-helper -n kube-system
NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
cni-metrics-helper   1/1     1            1           60s
</code></pre>

</section><section>

<h3 id="cni-metrics-helper-for-amazon-eks-cluster">CNI Metrics Helper for Amazon EKS Cluster</h3>

<p><img src="images/cni-metrics-helper.png"/></p>

<!--
![](https://docs.aws.amazon.com/eks/latest/userguide/images/EKS_CNI_metrics.png)
-->

</section><section>

<h2 id="section-2-2">Section 2.2:</h2>

<h2 id="networking-3">Networking</h2>

<h2 id="coredns-dns-server">CoreDNS (DNS Server)</h2>

</section><section>

<h3 id="coredns">CoreDNS</h3>

<ul>
<li>CoreDNS was GA in 1.11</li>
<li>CoreDNS uses <a href="https://coredns.io/2017/07/23/corefile-explained/">Corefile</a> for configuration</li>
<li>Corefile can be edited by editing coredns configmap</li>
</ul>

</section><section>

<h3 id="check-coredns-pods-are-running">Check CoreDNS pods are running</h3>

<pre><code>$ kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
---
NAME                       READY   STATUS    RESTARTS   AGE
coredns-79d667b89f-hcwnr   1/1     Running   0          63d
coredns-79d667b89f-qh8cd   1/1     Running   0          63d
</code></pre>

<h3 id="check-if-coredns-service-is-up">Check if CoreDNS service is up</h3>

<pre><code>$ kubectl get svc --namespace=kube-system
---
kubectl get svc kube-dns --namespace=kube-system             
NAME       TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.100.0.10   &lt;none&gt;        53/UDP,53/TCP   63d
</code></pre>

</section><section>

<h2 id="enable-logging-in-coredns">Enable logging in CoreDNS</h2>

<p>Add <strong>log</strong> in Corefile</p>

<pre><code>$ kubectl -n kube-system edit configmap coredns
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
    namespace: kube-system
    data:
      Corefile: |
      .:53 {
          log
          errors
          health
          kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            upstream
            fallthrough in-addr.arpa ip6.arpa
            } 
          prometheus :9153
          proxy . /etc/resolv.conf
          cache 30
          loop
          reload
          loadbalance
      }
</code></pre>

</section><section>

<h3 id="check-coredns-logs">Check CoreDNS logs</h3>

<pre><code>$ for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl logs --namespace=kube-system $p; done

.:53
2019-09-12T1647.907Z [INFO] CoreDNS-1.2.6
2019-09-12T1647.907Z [INFO] linux/amd64, go1.10.8, 756749c5
CoreDNS-1.2.6
linux/amd64, go1.10.8, 756749c5
 [INFO] plugin/reload: Running configuration MD5 = 2e2180a5eeb3ebf92a5100ab081a6381
 W1002 0656.326426       1 reflector.go:341] github.com/coredns/coredns/plugin/kubernetes/controller.go watch of *v1.Namespace ended with: too old resource version: 123473 (4874440)
 W1002 2207.890987       1 reflector.go:341] github.com/coredns/coredns/plugin/kubernetes/controller.go watch of *v1.Namespace ended with: too old resource version: 4874440 (5041806)
 W1002 2207.893920       1 reflector.go:341] github.com/coredns/coredns/plugin/kubernetes/controller.go watch of *v1.Service ended with: too old resource version: 123345 (5041806)
 W1008 0053.440168       1 reflector.go:341] github.com/coredns/coredns/plugin/kubernetes/controller.go watch of *v1.Service ended with: too old resource version: 5041806 (6327012)
</code></pre>

</section><section>

<h3 id="coredns-scaling">CoreDNS scaling</h3>

<ul>
<li>Memory required in MB</li>
</ul>

<blockquote>
<p>(Pods + Services)/1000 + 54</p>
</blockquote>

<ul>
<li><p>Amazon EKS configuration:</p>

<pre><code>Limits:
memory:  170Mi
Requests:
cpu:        100m
memory:     70Mi
</code></pre></li>

<li><p>Node-local DNS <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns">addon</a></p>

<ul>
<li>CoreDNS DaemonSet on each node</li>
</ul></li>

<li><p>Scale CoreDNS pods</p>

<pre><code>kubectl -n kube-system scale --replicas=10 deployment/coredns
</code></pre></li>
</ul>

<aside class="notes"><p>In large scale Kubernetes clusters, CoreDNS’s memory usage is predominantly affected by the number of Pods and Services in the cluster.</p>
</aside>

</section><section>

<h3 id="coredns-autopath-plugin">CoreDNS <em>autopath</em> plugin</h3>

<ul>
<li>Optional CoreDNS plugin</li>
<li>Improves performance for queries of names external to the cluster</li>
<li>Requires CoreDNS to use more memory</li>
</ul>

<blockquote>
<p>coreDNS Memory with <strong>autopath</strong> required in MB =</p>

<p>(Pods + Services)/250 + 56</p>
</blockquote>

</section><section>

<h2 id="section-2-3">Section 2.3:</h2>

<h2 id="networking-4">Networking</h2>

<h2 id="amazon-eks-endpoint-access">Amazon EKS Endpoint access</h2>

</section><section>

<h3 id="amazon-eks-cluster-endpoint-public-or-private">Amazon EKS Cluster Endpoint - public or private</h3>

<pre><code>$ kubectl get nodes
Unable to connect to the server: dial tcp: lookup BD969A3FAD4BC772192A7E99B5794C2F.gr7.us-east-1.eks.amazonaws.com: no such host
</code></pre>

</section><section>

<p><img src="images/eks-api-server-access.png" height="500"/></p>

</section>
</section>
    <section><section data-shortcode-section>
<h2 id="section-3">Section 3:</h2>

<h2 id="using-kubectl">Using Kubectl</h2>

</section><section>

<h4 id="eks-architecture">EKS Architecture</h4>

<p><img src="https://d1.awsstatic.com/Getting%20Started/eks-project/EKS-demo-app.e7ce7b188f2662b8573b5881a6b843e09caf729a.png" height="500" /></p>

</section><section>

<h3 id="how-does-kubectl-work">How does kubectl work</h3>

<p><span class='fragment '
  >
  - kubectl communicates with the Kubernetes API server
</span></p>

<p><span class='fragment '
  >
  - uses a configuration file generally located at ~/.kube/config
</span></p>

<p><span class='fragment '
  >
  - In EKS, kubectl &#43; aws-iam-authenticator = ❤️
</span></p>

</section><section>

<h3 id="kubectl-on-client-side">Kubectl on client-side</h3>

<p><strong>Validation</strong></p>

<ul>
<li><p>Infer generators, explicitly specified using <em>&ndash;generator</em></p></li>

<li><p>Create a Runtime object using generators</p></li>

<li><p>API version negotiation</p></li>

<li><p>REST request created</p></li>

<li><p>Authn</p></li>
</ul>

</section><section>

<h3 id="kubectl-on-server-side">Kubectl on server-side</h3>

<ul>
<li><p>Authn and authz</p></li>

<li><p>Admission controllers</p></li>

<li><p>Deserializes HTTP request to etcd</p></li>

<li><p>Initializers</p></li>

<li><p>Deployment, ReplicaSet, Scheduler controller</p></li>

<li><p>Kubelet queries API server (every 20 secs) for pods</p></li>

<li><p>Identify CRI and &ldquo;pause&rdquo; containers</p></li>

<li><p>CNI plugin (IP address allocation)</p></li>

<li><p>Container startup (pull image, start using CRI)</p></li>
</ul>

<aside class="notes"><p><a href="https://github.com/jamiehannaford/what-happens-when-k8s">https://github.com/jamiehannaford/what-happens-when-k8s</a></p>

<ul>
<li><p>kube-apiserver exposes its schema document (in OpenAPI format) at this path, it&rsquo;s easy for clients to perform their own discovery. To improve performance, kubectl also caches the OpenAPI schema to the ~/.kube/cache/discovery directory.</p></li>

<li><p>&ldquo;pause&rdquo; container which hosts all of the namespaces to allow inter-pod communication.</p></li>
</ul>
</aside>

</section><section>

<h3 id="have-you-ever-ran-into-this">&hellip; have you ever ran into this?</h3>

<pre><code>$ kubectl get svc 
error: the server doesn't have a resource type &quot;svc&quot;
</code></pre>

<ol>
<li><p>Update <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a> &amp; <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">aws-iam-authenticator</a></p></li>

<li><p>Update kubeconfig using AWS cli or eksctl</p></li>

<li><p>Is cluster endpoint accessible?</p></li>
</ol>

</section><section>

<h3 id="check-if-cluster-is-accessible">Check if cluster is accessible</h3>

<pre><code>$ curl -k http://CLUSTER_ENDPOINT/api/v1
</code></pre>

<p>response:</p>

<pre><code>&quot;kind&quot;: &quot;APIResourceList&quot;,
  &quot;groupVersion&quot;: &quot;v1&quot;,
  &quot;resources&quot;: [
  {
        &quot;name&quot;: &quot;bindings&quot;,
              &quot;singularName&quot;: &quot;&quot;,
                    &quot;namespaced&quot;: true,
                          &quot;kind&quot;: &quot;Binding&quot;,
                          &quot;verbs&quot;: [
                                  &quot;create&quot;
...                                        
</code></pre>

</section><section>

<h4 id="use-aws-cli-to-auto-generate-kube-config-file">Use AWS CLI to auto-generate kube config file</h4>

<pre><code>$ aws eks update-kubeconfig --name {cluster-name} 
</code></pre>

</section><section>

<pre><code>$ cat ~/.kube/config
-----
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: {REDACTED} 
    server: https://DFEA886AB17A069545SJDS9F06BCE3DCC.gr7.us-west-2.eks.amazonaws.com
  name: arneks09123456789:cluster/eks1
contexts:
- context:
    cluster: arneks09123456789:cluster/eks1
    user: arneks09123456789:cluster/eks1
  name: arneks09123456789:cluster/eks1
current-context: arneks09123456789:cluster/eks1
kind: Config
preferences: {}
users:
    - name: arneks09123456789:cluster/eks1
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - token
      - -i
      - eks1
      command: aws-iam-authenticator
</code></pre>

</section><section>

<h3 id="aws-auth-config-map">aws-auth config map</h3>

<pre><code>$ kubectl -n kube-system describe configmap aws-auth
-----
Name:         aws-auth
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;

Data
====
mapRoles:
----
- groups:
  - system:bootstrappers
  - system:nodes
  rolearn: arniam:role/eksctl-eks-nodegroup-ng
  username: system{{EC2PrivateDNSName}}

mapUsers:
----
- userarn: arniam:user/realvarez
  groups:
    - system:masters

</code></pre>

</section><section>

<p>If you are not part of the aws-auth configmap, then you&rsquo;ll see this,</p>

<pre><code>error: You must be logged in to the server (Unauthorized)
</code></pre>

<p>get your arn added to the aws-auth configmap</p>

<pre><code>mapUsers:
 ----
  - userarn: arniam:user/{YOUR_USER_ARN_HERE}
  -    groups:
  -         - system:masters
  -          
</code></pre>

</section><section>

<h3 id="kubectl-works">kubectl works!</h3>

<pre><code>$ kubectl cluster-info
-----
Kubernetes master is running at https://xxxx.y.region.eks.amazonaws.com
</code></pre>

<p>check cluster status:</p>

<pre><code>$ kubectl get componentstatus 
-----
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}   

</code></pre>

</section>
</section>
    <section><section data-shortcode-section>
<h2 id="section-4">Section 4:</h2>

<h2 id="pod-lifecycle">Pod Lifecycle</h2>

</section><section>

<h3 id="pod-lifecycle-1">Pod lifecycle</h3>

<p>You created a deployment with 8 replicas:</p>

<pre><code>$ kubectl create -f hello-deployment.yaml 
deployment.apps/hello created
</code></pre>

<div class="fragment" data-fragment-index="1" align=left>
<p>Or scaled an existing deployment to 8 replicas:</p>
</div>

<pre class="fragment bash" data-fragment-index="1" style="background-color:#f0f0f0" >
<code class="hljs shell">$ kubectl scale --replicas=8 deployment hello 
deployment.extensions/hello scaled
</code></pre>

<p><div class="fragment" data-fragment-index="2">
<p>Deployment shows only 4 replicas are available:</p>
</div>
<pre class="fragment" data-fragment-index="2" style="background-color:#f0f0f0" >
<code class="hljs shell">$ kubectl get deployments
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
hello   <sup>4</sup>&frasl;<sub>8</sub>     8            4           23s</code></pre></p>

</section><section>

<h3 id="list-pods">List pods</h3>

<pre><code>$ kubectl get pods

. . .

=======
NAME                     READY   STATUS    RESTARTS   AGE
hello-6d4fbd5d76-9xqxg   1/1     Running   0          5s
hello-6d4fbd5d76-brv7k   0/1     Pending   0          5s
hello-6d4fbd5d76-hbf8h   0/1     Pending   0          5s
hello-6d4fbd5d76-jdzlw   1/1     Running   0          5s
hello-6d4fbd5d76-jqsfk   0/1     Pending   0          5s
hello-6d4fbd5d76-k29gb   1/1     Running   0          5s
hello-6d4fbd5d76-vjr62   0/1     Pending   0          5s
hello-6d4fbd5d76-z69pp   1/1     Running   0          5s
</code></pre>

</section><section>

<h3 id="multiple-reasons-for-pending-pod">Multiple reasons for pending pod</h3>

<ul>
<li>Not enough resources in the cluster

<ul>
<li>CPU, memory, port</li>
</ul></li>
<li>Node security group does not have an ingress rule with 443 port access</li>
<li>Not enough IP addresses</li>
<li>Ensure all nodes are healthy</li>
</ul>

</section><section>

<h3 id="describe-the-pod">Describe the pod</h3>

<pre><code>$ kubectl describe pod/hello-6d4fbd5d76-brv7k
</code></pre>

<p>Shows the output:</p>

<pre><code>. . .

Events:
  Type     Reason             Age                From                Message
  ----     ------             ----               ----                -------
  Warning  FailedScheduling   42s (x2 over 42s)  default-scheduler   0/4 nodes are available: 4 Insufficient cpu.
</code></pre>

<p>Events are only visible on pods, not on Deployments, ReplicaSet, Job, or any other resource that created pod.</p>

</section><section>

<h3 id="get-all-events">Get all events</h3>

<pre><code>$ kubectl get events 
LAST SEEN   TYPE      REASON             KIND   MESSAGE
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
2m57s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
</code></pre>

</section><section>

<h3 id="only-the-warning-events">Only the warning events</h3>

<pre><code>$ kubectl get events --field-selector type=Warning
</code></pre>

</section><section>

<h3 id="events-only-for-the-pod">Events only for the pod</h3>

<pre><code>$ kubectl get events --field-selector involvedObject.kind=Pod,involvedObject.name=hello-6d4fbd5d76-brv7k
LAST SEEN   TYPE      REASON             KIND   MESSAGE
4m41s       Warning   FailedScheduling   Pod    0/4 nodes are available: 4 Insufficient cpu.
</code></pre>

</section><section>

<h3 id="sort-by-timestamp">Sort by timestamp</h3>

<pre><code>$ kubectl get events --sort-by='.lastTimestamp'
</code></pre>

</section><section>

<h3 id="memory-cpu-requirements-of-pod">Memory/CPU requirements of pod</h3>

<pre><code>$ kubectl describe deployments/hello
</code></pre>

<p>Output:</p>

<pre><code>  Containers:
   hello:
    Image:      nginx:latest
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Limits:
      cpu:     2
      memory:  2000Mi
    Requests:
      cpu:        2
      memory:     2000Mi
    Environment:  &lt;none&gt;
</code></pre>

</section><section>

<h3 id="request-and-limit">Request and limit</h3>

<p>Default CPU request is <strong>200m</strong> and none on memory. There are no limits.</p>

<pre><code>1000m (milicores) = 1 core = 1 CPU = 1 AWS vCPU
</code></pre>

<p>So, that means:</p>

<pre><code>100m cpu = 0.1 cpu
</code></pre>

<p>In this case, CPU request and limits have been specified to <strong>2</strong> and memory to <strong>2GB</strong>. So we need 8 blocks of 2 CPU and 2 GB memory.</p>

</section><section>

<h3 id="memory-cpu-in-cluster">Memory/CPU in cluster</h3>

<pre><code>$ kubectl top nodes
Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)
</code></pre>

</section><section>

<h3 id="install-metrics-server">Install metrics-server</h3>

<pre><code>$ curl -OL https://github.com/kubernetes-sigs/metrics-server/archive/v0.3.6.tar.gz
$ tar xzvf v0.3.6.tar.gz
$ kubectl create -f metrics-server-0.3.6/deploy/1.8+/
</code></pre>

</section><section>

<h3 id="confirm-metrics-api-is-available">Confirm Metrics API is available</h3>

<pre><code>$ kubectl get apiservice v1beta1.metrics.k8s.io
NAME                     SERVICE                      AVAILABLE   AGE
v1beta1.metrics.k8s.io   kube-system/metrics-server   True        11d
</code></pre>

</section><section>

<h3 id="memory-cpu-for-nodes">Memory/CPU for nodes</h3>

<pre><code>$ kubectl top nodes
NAME                                           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
ip-192-168-28-108.us-west-2.compute.internal   28m          0%     410Mi           2%        
ip-192-168-48-190.us-west-2.compute.internal   33m          0%     363Mi           2%        
ip-192-168-51-148.us-west-2.compute.internal   29m          0%     338Mi           2%        
ip-192-168-64-166.us-west-2.compute.internal   32m          0%     395Mi           2% 
</code></pre>

</section><section>

<h3 id="capacity-and-allocatable-memory-for-each-node">Capacity and allocatable memory for each node</h3>

<p><em>capacity</em> memory:</p>

<pre><code>$ kubectl get no -o json | jq -r '.items | sort_by(.status.capacity.memory)[]|[.metadata.name,.status.capacity.memory]| @tsv'
ip-192-168-28-108.us-west-2.compute.internal  15950552Ki
ip-192-168-48-190.us-west-2.compute.internal  15950552Ki
ip-192-168-51-148.us-west-2.compute.internal  15950552Ki
ip-192-168-64-166.us-west-2.compute.internal  15950552Ki
</code></pre>

<p><em>allocatable</em> memory:</p>

<pre><code>$ kubectl get no -o json | jq -r '.items | sort_by(.status.allocatable.memory)[]|[.metadata.name,.status.allocatable.memory]| @tsv'
ip-192-168-28-108.us-west-2.compute.internal  15848152Ki
ip-192-168-48-190.us-west-2.compute.internal  15848152Ki
ip-192-168-51-148.us-west-2.compute.internal  15848152Ki
ip-192-168-64-166.us-west-2.compute.internal  15848152Ki
</code></pre>

</section><section>

<h3 id="how-is-allocatable-calculated">How is allocatable calculated?</h3>

<pre><code>[Allocatable] = [Node Capacity] - [Kube-Reserved] - [System-Reserved] - [Hard-Eviction-Threshold]
</code></pre>

<p><span class='fragment '
  >
  Explained at <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md</a>.
</span></p>

</section><section>

<h3 id="capacity-and-allocatable-cpu">Capacity and allocatable CPU</h3>

<p><em>capacity</em> CPU:</p>

<pre><code>$ kubectl get no -o json | jq -r '.items | sort_by(.status.capacity.cpu)[]|[.metadata.name,.status.capacity.cpu]| @tsv'
ip-192-168-28-108.us-west-2.compute.internal  4
ip-192-168-48-190.us-west-2.compute.internal  4
ip-192-168-51-148.us-west-2.compute.internal  4
ip-192-168-64-166.us-west-2.compute.internal  4
</code></pre>

<p><em>allocatable</em> CPU:</p>

<pre><code>$ kubectl get no -o json | jq -r '.items | sort_by(.status.allocatable.cpu)[]|[.metadata.name,.status.allocatable.cpu]| @tsv'
ip-192-168-28-108.us-west-2.compute.internal  4
ip-192-168-48-190.us-west-2.compute.internal  4
ip-192-168-51-148.us-west-2.compute.internal  4
ip-192-168-64-166.us-west-2.compute.internal  4
</code></pre>

<p>So, there is enough memory and CPU. Why the pods are not getting scheduled?</p>

</section><section>

<h3 id="kubereserved-and-evictionhard-need-to-be-set-explicitly">kubeReserved and evictionHard need to be set explicitly</h3>

<p>EKS AMI now sets a minimum <code>evictionHard</code> and <code>kubeReserved</code> values: <a href="https://github.com/awslabs/amazon-eks-ami/pull/350">https://github.com/awslabs/amazon-eks-ami/pull/350</a>.</p>

<p>Alternatively, you can set these values using eksctl <a href="https://eksctl.io/usage/customizing-the-kubelet/">https://eksctl.io/usage/customizing-the-kubelet/</a>.</p>

</section><section>

<h3 id="cluster-autoscaler">Cluster Autoscaler</h3>

<p>Serves two purpose:</p>

<ul>
<li>Pods fail to run due to insufficient resources</li>
<li>Recycle nodes that are underutilized for an extended period of time</li>
</ul>

<p>Let&rsquo;s install it!</p>

</section><section>

<h3 id="iam-policy-for-cluster-autoscaler">IAM policy for Cluster Autoscaler</h3>

<p>Create IAM policy with autoscaling permissions and attach to the worker node IAM roles.</p>

<p>Create IAM policy:</p>

<pre><code>$ aws iam create-policy --policy-name AmazonEKSAutoscalingPolicy --policy-document file://../../resources/manifests/autoscaling-policy.json
{
    &quot;Policy&quot;: {
        &quot;PolicyName&quot;: &quot;AmazonEKSAutoscalingPolicy&quot;,
        &quot;PolicyId&quot;: &quot;ANPARKOFJSCVVWD4MQEKB&quot;,
        &quot;Arn&quot;: &quot;arniam:policy/AmazonEKSAutoscalingPolicy&quot;,
        . . .
    }
}
</code></pre>

<p>Attach policy to the IAM role:</p>

<pre><code>$ ROLE_NAME=$(aws iam list-roles \
  --query \
  'Roles[?contains(RoleName,`debug-k8s-nodegroup`)].RoleName' --output text)
$ aws iam attach-role-policy \
  --role-name $ROLE_NAME \
  --policy-arn arniam:policy/AmazonEKSAutoscalingPolicy
</code></pre>

</section><section>

<h3 id="auto-discovery-of-asg-by-ca">Auto-discovery of ASG by CA</h3>

<p>Setup auto discovery of Auto Scaling Groups by Cluster Autoscaler by attaching tags to the nodegroup:</p>

<pre><code>$ ASG_NAME=$(aws autoscaling describe-auto-scaling-groups \
  --query \
  'AutoScalingGroups[?contains(AutoScalingGroupName,`debug-k8s-nodegroup`)].AutoScalingGroupName' --output text)
$ aws autoscaling create-or-update-tags \
  --tags \
  ResourceId=$ASG_NAME,ResourceType=auto-scaling-group,Key=k8s.io/cluster-autoscaler/enabled,Value=something,PropagateAtLaunch=true \
  ResourceId=$ASG_NAME,ResourceType=auto-scaling-group,Key=k8s.io/cluster-autoscaler/debug-k8s,Value=something,PropagateAtLaunch=true
</code></pre>

</section><section>

<h3 id="create-cluster-autoscaler">Create Cluster Autoscaler:</h3>

<pre><code>$ CA_FILE=cluster-autoscaler-autodiscover.yaml
$ curl -o ${CA_FILE} https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml
$ sed -i -e 's/&lt;YOUR CLUSTER NAME&gt;/debug-k8s/' ${CA_FILE}
$ kubectl create -f ${CA_FILE}
</code></pre>

</section><section>

<h3 id="cluster-autoscaler-logs">Cluster Autoscaler logs</h3>

<pre><code>$ kubectl logs -f deployment/cluster-autoscaler -n kube-system
</code></pre>

<p>Shows the output:</p>

<pre><code>I1113 0100.754350       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-hbf8h is unschedulable
I1113 0100.754358       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-vjr62 is unschedulable
I1113 0100.754365       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-brv7k is unschedulable
I1113 0100.754371       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-jqsfk is unschedulable
I1113 0100.754407       1 scale_up.go:300] Upcoming 0 nodes
I1113 0100.754416       1 scale_up.go:335] Skipping node group eksctl-debug-k8s-nodegroup-ng-bb0efd30-NodeGroup-H77X21MZFGGH - max size reached
I1113 0100.754426       1 scale_up.go:411] No expansion options
</code></pre>

</section><section>

<h3 id="asg-limits">ASG Limits</h3>

<p>Update Autoscaling Group limits:</p>

<pre><code>$ aws autoscaling update-auto-scaling-group --auto-scaling-group-name $ASG_NAME --max-size 8
</code></pre>

<p>Cluster Autoscaler logs are updated:</p>

<pre><code>I1113 0111.009046       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-jqsfk is unschedulable
I1113 0111.009052       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-hbf8h is unschedulable
I1113 0111.009057       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-vjr62 is unschedulable
I1113 0111.009062       1 scale_up.go:263] Pod default/hello-6d4fbd5d76-brv7k is unschedulable
I1113 0111.009098       1 scale_up.go:300] Upcoming 0 nodes
I1113 0111.009322       1 waste.go:57] Expanding Node Group eksctl-debug-k8s-nodegroup-ng-bb0efd30-NodeGroup-H77X21MZFGGH would waste 50.00% CPU, 93.58% Memory, 71.79% Blended
I1113 0111.009346       1 scale_up.go:418] Best option to resize: eksctl-debug-k8s-nodegroup-ng-bb0efd30-NodeGroup-H77X21MZFGGH
I1113 0111.009356       1 scale_up.go:422] Estimated 4 nodes needed in eksctl-debug-k8s-nodegroup-ng-bb0efd30-NodeGroup-H77X21MZFGGH
I1113 0111.009374       1 scale_up.go:501] Final scale-up plan: [{eksctl-debug-k8s-nodegroup-ng-bb0efd30-NodeGroup-H77X21MZFGGH 4-&gt;8 (max: 8)}]
</code></pre>

</section><section>

<h3 id="check-pods">Check pods</h3>

<pre><code>$ kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
hello-6d4fbd5d76-9xqxg   1/1     Running   0          6m30s
hello-6d4fbd5d76-brv7k   1/1     Running   0          6m30s
hello-6d4fbd5d76-hbf8h   1/1     Running   0          6m30s
hello-6d4fbd5d76-jdzlw   1/1     Running   0          6m30s
hello-6d4fbd5d76-jqsfk   1/1     Running   0          6m30s
hello-6d4fbd5d76-k29gb   1/1     Running   0          6m30s
hello-6d4fbd5d76-vjr62   1/1     Running   0          6m30s
hello-6d4fbd5d76-z69pp   1/1     Running   0          6m30s
</code></pre>

</section><section>

<h3 id="similar-usecase">Similar usecase</h3>

<pre><code>$ kubectl get pods -l app=mnist,type=inference
NAME                             READY   STATUS    RESTARTS   AGE
mnist-inference-cd78cfd5-hcvfd   0/1     Pending   0          3m48s
</code></pre>

</section><section>

<h3 id="similar-analysis">Similar analysis</h3>

<p>Get details about the pod:</p>

<pre><code>$ kubectl describe pod mnist-inference-cd78cfd5-hcvfd

. . .

Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  3s (x8 over 5m32s)  default-scheduler  0/2 nodes are available: 2 Insufficient nvidia.com/gpu.
</code></pre>

</section><section>

<p>Need to create a cluster with more GPUs.</p>

</section>
</section>
    <section><section data-shortcode-section>
<h2 id="section-5">Section 5:</h2>

<h2 id="kubelet-resource-reservation">Kubelet Resource Reservation</h2>

</section><section>

<h3 id="kubelet-resource-reservation-1">Kubelet resource reservation</h3>

<ul>
<li><p>Monitor kubelet on the worker node</p>

<pre><code>journalctl -u kubelet
</code></pre></li>

<li><p>Use kube-reserved to reserve resources for kubelet, container runtime &amp; node problem detector</p>

<pre><code>--kube-reserved=[cpu-100m][,][memory=100Mi][,]
[ephemeral-storage=1Gi][,][pid=1000]
</code></pre></li>

<li><p>Use system-reserved to reserve resources for system daemons liks sshd, udev, kernel</p>

<pre><code>--system-reserved=[cpu-100m][,][memory=100Mi][,]
[ephemeral-storage=1Gi][,][pid=1000]
</code></pre></li>
</ul>

</section><section>

<h3 id="kubelet-resource-reservation-for-amazon-eks-using-user-data">Kubelet resource reservation for Amazon EKS using User Data</h3>

<pre><code>ADD EXAMPLE
</code></pre>

</section><section>

<h3 id="kubelet-resource-reservation-for-amazon-eks-using-eksctl">Kubelet resource reservation for Amazon EKS using eksctl</h3>

<pre><code>apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: myeks
  region: us-east-1

nodeGroups:
  - name: myng
    instanceType: m5.xlarge
    desiredCapacity: 4
    kubeletExtraConfig:
        kubeReserved:
            cpu: &quot;300m&quot;
            memory: &quot;300Mi&quot;
            ephemeral-storage: &quot;1Gi&quot;
        kubeReservedCgroup: &quot;/kube-reserved&quot;
        systemReserved:
            cpu: &quot;300m&quot;
            memory: &quot;300Mi&quot;
            ephemeral-storage: &quot;1Gi&quot;
        evictionHard:
            memory.available:  &quot;200Mi&quot;
            nodefs.available: &quot;10%&quot;
        featureGates:
            DynamicKubeletConfig: true
            RotateKubeletServerCertificate: true # has to be enabled, otherwise it will be disabled
</code></pre>

</section><section>

<h3 id="avoid-oversubscription">Avoid oversubscription</h3>

<ul>
<li><p>Use resource limits and requests</p>

<pre><code>Pod.sepc.containers.resources: 
                      limits: 
                        memory: 128Mi 
                        cpu: 500m 
                      requests:
                        memory: 64Mi 
                        cpu: 250m
</code></pre></li>

<li><p>Use resource quotas on namespaces</p>

<pre><code>apiVersion: v1 
kind: ResourceQuota 
metadata: 
  name: resource-quota 
    namespace: quota-ns
spec: 
  hard: 
      limits.cpu: 2
</code></pre></li>
</ul>

</section><section>

<p>Once the quota on the namespace is full, newer pods will fail</p>

<pre><code>$ kubectl  get events
...
Error creating: pods &quot;nginx-fb556779d-xmsgv&quot; is forbidden: 
exceeded quota: resource-quota, requested: limits.cpu=500m, used: limits.cpu=2, limited: limits.cpu=2
</code></pre>

</section><section>

<h3 id="use-limitrange-to-set-default-limit">Use <strong>LimitRange</strong> to set default limit</h3>

<pre><code>apiVersion: v1 
kind: LimitRange 
metadata: 
  name: limit-range-example 
    namespace: limit-range-example 
    spec: 
      limits: 
        - default: 
            memory: 512Mi 
            cpu: 1 
          defaultRequest: 
            memory: 256Mi 
            cpu: 500m 
          type: Container
</code></pre>

<aside class="notes"><p>“When you are using quotas on a namespace, one requirement is that every container in the namespace must have resource limits and requests defined. Sometimes this requirement can cause complexity and make it more difficult to work quickly with Kubernetes. Specifying resource limits correctly, while an essential part of preparing an application for production, does add additional overhead when, for example, using Kubernetes as a platform for development or testing workloads.”
<aside class="notes"></section><section></p>

<pre><code>$ kubectl describe limitranges 
Name:       limit-mem-cpu-per-container
Namespace:  limitrange
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -
</code></pre>

</section><section>

<h3 id="limits-scenarios">Limits scenarios</h3>

<p>(container limits and requests override Limitranges)</p>

<p><ol>
<li>If container defines both <em>limits</em> and <em>requests</em>, <em>LimitRanges</em> will have no effect</li>
<li>If container has no <em>limits</em> or <em>requests</em>, <em>LimitRanges</em> will be inherited</li>
<li>If container has <em>requests</em> but not <em>limits</em>, <em>limits</em> will be inherited from <em>LimitRanges</em></li>
<li>If container has <em>limits</em> defined, then <em>requests</em> = <em>limits</em>, <em>LimitRanges</em> will have no effect</li>
</ol>
</aside></p>
</aside>

</section>
</section>
    <section><section data-shortcode-section>
<h2 id="section-6">Section 6:</h2>

<h2 id="stateful-containers">Stateful Containers</h2>

</section><section>

<h3 id="pods-are-ephemeral-by-nature">Pods are ephemeral by nature</h3>

<ul>
<li><p>Volumes allow pods to persist data</p></li>

<li><p>Volumes are accessible to all containers in a pod</p></li>

<li><p>Data can persist even after pod termination</p></li>
</ul>

</section><section>

<h3 id="persistentvolume-pv">PersistentVolume (PV)</h3>

<blockquote>
<p>pre-provisioned storage in the cluster or dynamically provisioned by storage class</p>
</blockquote>

<h3 id="persistentvolumeclaim-pvc">PersistentVolumeClaim (PVC)</h3>

<blockquote>
<p>request for storage by a user</p>
</blockquote>

<h3 id="storageclass">StorageClass</h3>

<blockquote>
<p>administrator provided &ldquo;classes&rdquo; of storage</p>
</blockquote>

<aside class="notes"><p>Persistent Volumes provide a plugin model for storage in Kubernetes. How storage is provided is abstracted from how it is consumed</p>
</aside>

</section><section>

<h3 id="what-are-statefulsets">What are statefulsets?</h3>

<ul>
<li>Provide pods with storage to persist data</li>
<li>Create PVC dynamically using <code>volumeClaimTemplates</code></li>
<li>Each pod gets its own dedicated PVC</li>
<li>Create headless service type (<code>clusterIP: None</code>)</li>
<li>Ordered deployment and scaling</li>
</ul>

<aside class="notes"><p>“As you probably realize, we said PVCs are created and associated with the Pods, but we didn’t say anything about PVs. That is because StatefulSets do not manage PVs in any way. The storage for the Pods must be provisioned in advance by an admin, or provisioned on-demand by a PV provisioner based on the requested storage class and ready for consumption by the stateful Pods.”
<aside class="notes"></section><section></p>

<h3 id="step-1-create-storage-class">Step 1. Create storage class</h3>

<pre><code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: mysql-gp2
  provisioner: kubernetes.io/aws-ebs
  parameters:
    type: gp2
    reclaimPolicy: Delete
    mountOptions:
    - debug
</code></pre>

</section><section>

<h3 id="step-2-use-volumeclaimtemplates-in-the-statefulset">Step 2. use volumeClaimTemplates in the Statefulset</h3>

<pre><code>volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: mysql-gp2
      resources:
        requests:
          storage: 10Gi
</code></pre>

</section><section>

<h3 id="mysql-statefulsets-on-kubernetes">mysql statefulsets on Kubernetes</h3>

<pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  spec:
    selector:
      matchLabels:
        app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
   ...

        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql

  ...
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [&quot;ReadWriteOnce&quot;]
      storageClassName: mysql-gp2
      resources:
        requests:
          storage: 10Gi
</code></pre>

<p>See full yaml <a href="https://eksworkshop.com/statefulset/statefulset.files/mysql-statefulset.yml">here</a></p>

</section><section>

<h3 id="persistent-storage-options-in-eks">Persistent storage options in EKS</h3>

<ul>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html">EBS CSI Driver</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html">EFS CSI Driver</a></li>
<li><a href="https://github.com/kubernetes-sigs/aws-fsx-csi-driver">FSx for Lustre CSI Driver (Alpha)</a></li>
</ul>

</section><section>

<h3 id="eks-ebs">EKS + EBS</h3>

<ul>
<li>Persistent volumes can be <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/dynamic-provisioning">dynamically</a> or
<a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/static-provisioning">statically</a> provisioned</li>
<li>Volumes can be <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/resizing">resized</a> and support <a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes/snapshot">Snapshots</a></li>
<li>Volumes are local to AZ, use EFS where possible</li>
</ul>

</section><section>

<h3 id="question">Question:</h3>

<p>I mounted a volume in my Pod but it is read-only?</p>

<h3 id="answer">Answer:</h3>

<ul>
<li>Security Context for a Pod</li>

<li><p>initContainers</p>

<h3 id="why">Why?</h3></li>

<li><p>Volumes are mounted as root</p></li>
</ul>

</section><section>

<h3 id="that-leaves-you-with-two-options">That leaves you with two options</h3>

<p><ol>
<li><p>Setup <a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">Security context</a> in your pod</p></p>

<p><pre><code>apiVersion: v1
kind: Pod
metadata:
name: security-context-demo
spec:
securityContext:
  runAsUser: 1000
  runAsGroup: 3000
  fsGroup: 2000
</code></pre></li></p>

<p><li><p>Use <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">initContainers</a> to change permissions</p></li>
</ol>
</aside></p>
</aside>

</section>
</section>
    <section><section data-shortcode-section>
<h2 id="section-7">Section 7:</h2>

<h2 id="going-public-load-balancers-and-ingress">Going public, Load balancers and Ingress</h2>

</section><section>

<h3 id="options-for-exposing-kubernetes-apps">Options for exposing Kubernetes apps</h3>

<ul>
<li><p>ServiceType=LoadBalancer. Uses Classic load balancer.</p>

<ul>
<li><p>Use Network load balancer, annotate your service:</p>

<pre><code>service.beta.kubernetes.io/aws-load-balancer-type: nlb
</code></pre></li>
</ul></li>

<li><p>Ingress using <a href="https://github.com/kubernetes-sigs/aws-alb-ingress-controller">alb-ingresss-controller</a></p>

<ul>
<li><p>For internal only load balancer, annotate your service:</p>

<pre><code>service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0

</code></pre></li>
</ul></li>

<li><p><a href="https://github.com/kubernetes/ingress-nginx">NGINX Ingress Controller</a></p></li>
</ul>

</section><section>

<h3 id="if-load-balancer-fails-to-deploy">If load balancer fails to deploy</h3>

<ul>
<li>Check <a href="https://docs.aws.amazon.com/en_pv/eks/latest/userguide/network_reqs.html">tags</a></li>

<li><p>Check service</p>

<pre><code>kubectl describe service
</code></pre></li>

<li><p>Check <a href="https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html">EKS IAM Service Role</a></p></li>
</ul>

</section><section>

<h3 id="troubleshoot-latency-in-app">Troubleshoot latency in app</h3>

<ul>
<li>Check <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html">CloudWatch Metrics for load balancer</a></li>

<li><p>Check latency within cluster</p>

<pre><code>kubectl apply -f https://k8s.io/examples/application/shell-demo.yaml

kubectl exec -it shell-demo -- /bin/bash

root@shell-demo:/# apt-get update 
root@shell-demo:/# apt-get install curl
root@shell-demo:/# curl http://10.100.2.39 &lt;-- IP of the service
</code></pre></li>

<li><p>Implement <a href="https://docs.aws.amazon.com/whitepapers/latest/microservices-on-aws/distributed-monitoring.html">tracing</a>
preferably with a <a href="https://aws.amazon.com/app-mesh/">Service Mesh</a></p></li>
</ul>

</section>
</section>
    <section><section data-shortcode-section>
<h2 id="section-8">Section 8.</h2>

<h2 id="finding-problems">Finding Problems</h2>

</section><section>

<h3 id="what-metrics-to-monitor">What metrics to monitor</h3>

<ul>
<li>Nodes</li>
<li>Cluster components</li>
<li>Add-ons</li>
<li>Apps</li>
</ul>

</section><section>

<h3 id="key-metrics">Key metrics</h3>

<ul>
<li>Nodes

<ul>
<li>CPU, memory, network, disk</li>
</ul></li>
<li>Cluster Components

<ul>
<li>etcd</li>
<li>coredns latency</li>
</ul></li>
<li>Add-ons

<ul>
<li>Cluster auto scaler</li>
<li>Ingress controller</li>
</ul></li>
<li>Apps

<ul>
<li>Container CPU/memory/network utilization</li>
<li>Error rates</li>
<li>App specific metrics</li>
</ul></li>
</ul>

</section><section>

<h3 id="monitoring-tools">Monitoring tools</h3>

<ul>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/prometheus.html">Prometheus</a> + <a href="https://eksworkshop.com/monitoring/">Grafana</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/deploy-container-insights-EKS.html">CloudWatch Container Insights</a></li>
<li>Kubernetes <a href="https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html">Dashboard</a></li>
<li>Commercial solutions (e.g., Datadog, Dynatrace, Sysdig, etc.)</li>
</ul>

<aside class="notes"><p>Prometheus provides metrics storage. Grafana provides dashboards.</p>
</aside>

</section><section>

<h3 id="install-prometheus">Install Prometheus</h3>

<pre><code>$ kubectl create namespace prometheus
$ helm install stable/prometheus \
    --name prometheus \
    --namespace prometheus \
    --set alertmanager.persistentVolume.storageClass=&quot;gp2&quot; \
    --set server.persistentVolume.storageClass=&quot;gp2&quot;
</code></pre>

</section><section>

<h3 id="verify-prometheus">Verify Prometheus</h3>

<pre><code>$ kubectl -n prometheus get pods
prometheus-alertmanager-58c6876d4c-6p5qs              2/2     Running   0          20s
prometheus-kube-state-metrics-74b57df8bd-jksg8        1/1     Running   0          20s
prometheus-node-exporter-2nvkk                        1/1     Running   0          20s
prometheus-node-exporter-fzlts                        1/1     Running   0          20s
prometheus-node-exporter-s4nsc                        1/1     Running   0          20s
prometheus-pushgateway-c497ff84d-bxm5b                1/1     Running   0          20s
prometheus-server-796c867465-q775s                    2/2     Running   0          20s
</code></pre>

</section><section>

<h3 id="access-prometheus">Access Prometheus</h3>

<pre><code>$ kubectl port-forward -n prometheus deploy/prometheus-server 8080:9090
</code></pre>

<p><center>
<img src="https://eksworkshop.com/images/prometheus-targets.png" />
</center></p>

</section><section>

<h3 id="install-grafana">Install Grafana</h3>

<pre><code>$ kubectl create namespace grafana
$ helm install stable/grafana \
    --name grafana \
    --namespace grafana \
    --set persistence.storageClassName=&quot;gp2&quot; \
    --set adminPassword='EKS!sAWSome' \
    --set datasources.&quot;datasources\.yaml&quot;.apiVersion=1 \
    --set datasources.&quot;datasources\.yaml&quot;.datasources[0].name=Prometheus \
    --set datasources.&quot;datasources\.yaml&quot;.datasources[0].type=prometheus \
    --set datasources.&quot;datasources\.yaml&quot;.datasources[0].url=http://prometheus-server.prometheus.svc.cluster.local \
    --set datasources.&quot;datasources\.yaml&quot;.datasources[0].access=proxy \
    --set datasources.&quot;datasources\.yaml&quot;.datasources[0].isDefault=true \
    --set service.type=LoadBalancer
</code></pre>

</section><section>

<h3 id="verify-grafana-pods">Verify Grafana pods</h3>

<pre><code>$ kubectl -n grafana get pods
NAME                          READY     STATUS    RESTARTS   AGE
pod/grafana-b9697f8b5-t9w4j   1/1       Running   0          2m
</code></pre>

</section><section>

<h3 id="get-grafana-elb-url">Get Grafana ELB URL</h3>

<pre><code>$ export ELB=$(kubectl get svc \
    -n grafana grafana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
$ echo &quot;http://$ELB&quot;
</code></pre>

<p>Get password for admin user</p>

<pre><code>$ kubectl get secret --namespace grafana grafana \
      -o jsonpath=&quot;{.data.admin-password}&quot; | base64 --decode ; echo
</code></pre>

</section><section>

<h2 id="community-created-dashboard-3131">Community created Dashboard #3131</h2>

<p><img src="https://eksworkshop.com/images/grafana-all-nodes.png" /></p>

</section><section>

<h2 id="community-created-dashboard-3146">Community created Dashboard #3146</h2>

<p><img src="https://eksworkshop.com/images/grafana-all-pods.png" /></p>

</section><section>

<h3 id="important-log-files">Important log files</h3>

<ul>
<li>Node logs</li>
<li>Kubernetes control plane logs

<ul>
<li>API Server</li>
<li>Controller Manager</li>
<li>Scheduler</li>
</ul></li>
<li>Kubernetes audit logs</li>
<li>App container logs</li>
</ul>

</section><section>

<h3 id="cloudwatch-container-inights">CloudWatch Container Inights</h3>

<ul>
<li>Collect, aggregate, and summarize metrics</li>
<li>Capture logs (uses FluentD)</li>
<li>Get diagnostic information, such as container restart failures</li>
</ul>

</section><section>

<h3 id="cloudwatch-container-insights-components">Cloudwatch Container Insights components</h3>

<ol>
<li>CloudWatch agent daemonset</li>

<li><p>FluentD daemonset</p>

<pre><code>$ kubectl get ds -n amazon-cloudwatch
NAME                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
cloudwatch-agent     3         3         3       3            3           &lt;none&gt;          21h
fluentd-cloudwatch   3         3         3       3            3           &lt;none&gt;          21h
</code></pre></li>
</ol>

</section><section>

<h4 id="cloudwatch-container-insights-dashboard">CloudWatch Container Insights Dashboard</h4>

<p><center><img src=images/Container-insights.png height="75%" width="75%"></center></p>

</section><section>

<p><img src="images/Container-insights-pod.png" alt="" /></p>

</section><section>

<h3 id="using-cloudwatch-logs-insights">Using CloudWatch Logs Insights</h3>

<ul>
<li>Container Insights collects metrics by using performance log events</li>
<li>Durable, logs stored in CloudWatch Logs</li>
<li>CloudWatch Logs Insights queries for additional views of your container data</li>
</ul>

</section><section>

<h4 id="query-container-logs-and-kubernetes-metrics">Query container logs and Kubernetes metrics</h4>

<p><img src=images/Container-insights-logs.jpg /></p>

</section><section>

<h3 id="container-monitoring-options">Container monitoring options</h3>

<ul>
<li>Readiness probe</li>
<li>Liveness probe</li>
<li>Tracing (OpenTracing, AWS X-ray)</li>
<li>Process health</li>
<li>Process metrics</li>
<li>Process logs</li>
</ul>

<aside class="notes"><p>A good cloud native container must provide APIs for runtime to monitor the health, if checks fail, an action should be triggered.</p>
</aside>

</section>
</section>
    <section><section data-shortcode-section>
<h2 id="section-9">Section 9:</h2>

<h2 id="pod-scaling">Pod Scaling</h2>

</section><section>

<h3 id="connection-refused-to-pod">Connection refused to pod</h3>

<p>Let&rsquo;s look at a different part of the problem now.</p>

<p>Pods are all scheduled but not able to meet throughput and/or latency needs.</p>

</section><section>

<p>Create horizontal pod autoscaler</p>

<p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a> (HPA) scales the pods in a deployment or replica set. It is implemented as a K8s API resource and a controller. The controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. It obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).</p>

<p>Default HPA loop is 15 seconds (controlled by <code>--horizontal-pod-autoscaler-sync-period</code>)</p>

</section><section>

<p>Creata a Deployment and Service:</p>

<pre><code>kubectl create -f resources/manifests/hpa-example.yaml
</code></pre>

<p>The index.php page performs calculations to generate CPU load. More information can be found <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#run-expose-php-apache-server">here</a>.</p>

</section><section>

<p>Create a shell using a new container:</p>

<pre><code>kubectl run -i --tty load-generator --image=busybox /bin/sh
</code></pre>

</section><section>

<p>Execute a loop to invoke the service:</p>

<pre><code>while true; do wget -q -O - http://php-apache; done
</code></pre>

<p>Shows the output as:</p>

<pre><code>OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!wget: error getting response: No such file or directory
wget: can't connect to remote host (10.100.59.78): Connection refused
wget: can't connect to remote host (10.100.59.78): Connection refused
wget: can't connect to remote host (10.100.59.78): Connection refused
wget: can't connect to remote host (10.100.59.78): Connection refused
wget: can't connect to remote host (10.100.59.78): Connection refused
wget: can't connect to remote host (10.100.59.78): Connection refused
wget: can't connect to remote host (10.100.59.78): Connection refused
wget: can't connect to remote host (10.100.59.78): Connection refused
wget: can't connect to remote host (10.100.59.78): Connection refused
wget: can't connect to remote host (10.100.59.78): Connection refused
wget: can't connect to remote host (10.100.59.78): Connection refused
wget: can't connect to remote host (10.100.59.78): Connection refused
OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!
</code></pre>

</section><section>

<p>Create an autoscale deployment:</p>

<pre><code>$ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
</code></pre>

</section><section>

<p>Get details about HPA:</p>

<pre><code>$ kubectl get hpa
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   0%/50%    1         10        1          20s
</code></pre>

</section><section>

<p>Generate load to trigger scaling:</p>

<pre><code>kubectl run -i --tty load-generator --image=busybox /bin/sh
</code></pre>

<p>Execute a while loop to generate load:</p>

<pre><code>while true; do wget -q -O - http://php-apache; done
</code></pre>

</section><section>

<p>Get the current status of HPA:</p>

<pre><code>$ kubectl get hpa -w
NAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   0%/50%     1         10        1          20m
php-apache   Deployment/php-apache   167%/50%   1         10        4          21m
php-apache   Deployment/php-apache   128%/50%   1         10        4          22m
php-apache   Deployment/php-apache   128%/50%   1         10        8          22m
php-apache   Deployment/php-apache   128%/50%   1         10        10         22m
php-apache   Deployment/php-apache   46%/50%    1         10        10         23m
php-apache   Deployment/php-apache   0%/50%     1         10        10         25m
php-apache   Deployment/php-apache   0%/50%     1         10        1          29m
</code></pre>

</section><section>

<pre><code>/ # while true; do wget -q -O - http://php-apache; done
OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK! . . .
</code></pre>

</section><section>

</section>
</section>
    <section>

<h3 id="get-involved-contribute-to-this-presentation">Get Involved! Contribute to this presentation</h3>

<p><a href="https://github.com/aws-samples/debug-k8s-apps">https://github.com/aws-samples/debug-k8s-apps</a></p>

<h3 id="view-this-presentation">View this presentation</h3>

<p><a href="https://aws-samples.github.io/debug-k8s-apps/">https://aws-samples.github.io/debug-k8s-apps/</a></p>

</section><section>

<h3 id="links">Links</h3>

<p><a href="https://aws.amazon.com/blogs/opensource/">AWS Open Source Blog</a></p>

<p><a href="https://aws.amazon.com/blogs/containers/">AWS Container Blog</a></p>

<p><a href="https://eksworkshop.com">EKS Workshop</a></p>

<p><a href="https://k8s.af">Kubernetes Failure Stories</a></p>

</section><section>

<h3 id="further-reading">Further Reading</h3>

<table>
  <tr>
  <td><a href="https://www.oreilly.com/library/view/kubernetes-patterns/9781492050278/"><img src="https://www.safaribooksonline.com/library/cover/9781492050278/360h/" height=150 /></a>
  <font size=3><center><b>Kubernetes Patterns</b> <br>by Roland Huß, Bilgin Ibryam</center></font></td>

  <td><a href="https://www.oreilly.com/library/view/kubernetes-on-aws/9781788390071/"><img src="https://www.safaribooksonline.com/library/cover/9781788390071/360h/" height=150 /></a>
  <font size=3><center><b>Kubernetes on AWS</b><br>by Ed Robinson</center></font></td>

  <td><a href="https://www.oreilly.com/library/view/kubernetes-up-and/9781491935668/"><img src="https://www.safaribooksonline.com/library/cover/9781491935668/360h/" height=150 /></a>
  <font size=3><center><b>Kubernetes: Up and Running </b><br>by Joe Beda, Brendan Burns, Kelsey Hightower</center></font></td>

  <td><a href="https://www.oreilly.com/library/view/programming-kubernetes/9781492047094/"><img src="https://www.safaribooksonline.com/library/cover/9781492047094/360h/" height=150 /></a>
  <font size=3><center><b>Programming Kubernetes</b><br> by Stefan Schimanski, Michael Hausenblas</center></font></td>
  </tr>
</table>

<style>
table {
  width:80%;
  table-layout: fixed;
}
  td{width:100%;border: 0px solid black;}
</style>
</section>
    <section><p>Thank you!</p>
</section>

</div>
      
    </div>
<script type="text/javascript" src=/debug-k8s-apps/reveal-hugo/object-assign.js></script>

<a href="/debug-k8s-apps/reveal-js/css/print/" id="print-location" style="display: none;"></a>
<script type="text/javascript">
  var printLocationElement = document.getElementById('print-location');
  var link = document.createElement('link');
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = printLocationElement.href + (window.location.search.match(/print-pdf/gi) ? 'pdf.css' : 'paper.css');
  document.getElementsByTagName('head')[0].appendChild(link);
</script>

<script type="application/json" id="reveal-hugo-site-params">{"controls":false,"custom_theme":"stylesheets/robot-lung.css","custom_theme_compile":true,"slide_number":true}</script>
<script type="application/json" id="reveal-hugo-page-params">{"controls":true,"hash":false,"height":"89%","margin":0.1,"progress":false,"slide_number":true,"theme":"white","transition":"fade","width":"100%"}</script>

<script src="/debug-k8s-apps/reveal-js/js/reveal.js"></script>

<script type="text/javascript">
  
  
  function camelize(map) {
    if (map) {
      Object.keys(map).forEach(function(k) {
        newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
        if (newK != k) {
          map[newK] = map[k];
          delete map[k];
        }
      });
    }
    return map;
  }
  
  var revealHugoDefaults = { center: true, controls: true, history: true, progress: true, transition: "slide" };
  var revealHugoSiteParams = JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);
  var revealHugoPageParams = JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);
  
  var options = Object.assign({},
    camelize(revealHugoDefaults),
    camelize(revealHugoSiteParams),
    camelize(revealHugoPageParams));
  Reveal.initialize(options);
</script>


  
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/markdown/marked.js"></script>
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/markdown/markdown.js"></script>
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/highlight/highlight.js"></script>
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/zoom-js/zoom.js"></script>
  
  
  <script type="text/javascript" src="/debug-k8s-apps/reveal-js/plugin/notes/notes.js"></script>



    <style>
  #logo {
    position: absolute;
    top: 0px;
    left: 0px;
    width: 100%;
    z-index: -1;

  }
</style>

<img id="logo" src="images/kubecon-slide-header.png" alt="">

    
  </body>
</html>
